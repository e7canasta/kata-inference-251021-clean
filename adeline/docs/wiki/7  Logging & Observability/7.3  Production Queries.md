# Production Queries

Relevant source files

- [adeline/logging.py](https://github.com/acare7/kata-inference-251021-clean4/blob/master/adeline/logging.py)
- [adeline/CLAUDE.md](https://github.com/acare7/kata-inference-251021-clean4/blob/master/adeline/CLAUDE.md)

## Purpose and Scope

This document provides a cookbook of production-ready `jq` queries for analyzing Adeline's structured JSON logs. These queries enable rapid debugging, performance analysis, and operational monitoring without custom parsing scripts.

**Scope:**

- Common query patterns for debugging and monitoring
- Performance analysis queries (FPS, latency aggregation)
- Trace correlation and end-to-end request analysis
- Error analysis by component and type

**Prerequisites:**

- `jq` installed (`sudo apt install jq` on Debian/Ubuntu)
- JSON logs either in file or piped from stdout

**For related topics:**

- Log event schemas, see [Log Event Patterns](https://deepwiki.com/acare7/kata-inference-251021-clean4/7.2-log-event-patterns)
- Structured logging design, see [Structured Logging Design](https://deepwiki.com/acare7/kata-inference-251021-clean4/7.1-structured-logging-design)

---

## Basic Queries

### View All Logs (Pretty-Printed)

```bash
# File
cat logs/adeline.log | jq .

# Live stdout
python -m adeline | jq .
```

**Use Case:** Initial log inspection, development debugging.

---

### Filter by Log Level

```bash
# Errors only
jq 'select(.level == "ERROR")' logs/adeline.log

# Warnings and errors
jq 'select(.level == "WARNING" or .level == "ERROR")' logs/adeline.log

# Debug logs (development)
jq 'select(.level == "DEBUG")' logs/adeline.log
```

**Use Case:** Isolate errors for troubleshooting, filter out debug noise in production.

**Output Example:**

```json
{
  "timestamp": "2025-10-22T16:30:45.567890",
  "level": "ERROR",
  "logger": "adeline.control.plane",
  "message": "MQTT connection failed: Connection refused",
  "component": "control_plane",
  "error_type": "ConnectionRefusedError"
}
```

---

### Filter by Component

```bash
# Control Plane logs only
jq 'select(.component == "control_plane")' logs/adeline.log

# Data Plane logs only
jq 'select(.component == "data_plane")' logs/adeline.log

# Stabilization logs only
jq 'select(.component == "stabilization")' logs/adeline.log

# Inference Pipeline logs only
jq 'select(.component == "inference_pipeline")' logs/adeline.log
```

**Use Case:** Isolate logs by architectural boundary for component-specific debugging.

---

### Filter by Logger Name

```bash
# All control plane module logs
jq 'select(.logger | startswith("adeline.control"))' logs/adeline.log

# All inference module logs
jq 'select(.logger | startswith("adeline.inference"))' logs/adeline.log

# Specific module
jq 'select(.logger == "adeline.inference.stabilization.core")' logs/adeline.log
```

**Use Case:** Debugging specific modules within a component.

---

## Trace Correlation Queries

### Trace Specific Command End-to-End

```bash
# All logs for specific trace_id
TRACE_ID="cmd-pause-abc123"
jq "select(.trace_id == \"$TRACE_ID\")" logs/adeline.log

# Compact view (message only)
jq -r "select(.trace_id == \"$TRACE_ID\") | .message" logs/adeline.log
```

**Use Case:** End-to-end tracing of MQTT command through pipeline (command â†’ handler â†’ pipeline â†’ sinks).

**Output Example:**

```
ðŸ“¥ Comando recibido: pause
Ejecutando comando: pause
â¸ï¸ Command PAUSE received
âœ… Pipeline paused (use RESUME to continue)
```

---

### Find All Traces for Specific Command Type

```bash
# All pause commands
jq 'select(.command == "pause")' logs/adeline.log

# Extract unique trace_ids for pause commands
jq -r 'select(.command == "pause") | .trace_id' logs/adeline.log | sort -u

# Count pause commands
jq -r 'select(.command == "pause")' logs/adeline.log | wc -l
```

**Use Case:** Analyze frequency of specific commands, find traces for failed commands.

---

### Correlate Command to Errors

```bash
# All errors with trace_id set (correlated to commands)
jq 'select(.level == "ERROR" and .trace_id != null)' logs/adeline.log

# Errors for specific trace
TRACE_ID="cmd-pause-abc123"
jq "select(.trace_id == \"$TRACE_ID\" and .level == \"ERROR\")" logs/adeline.log
```

**Use Case:** Find which MQTT command triggered an error, debug command failures.

**Output Example:**

```json
{
  "timestamp": "2025-10-22T16:30:46.123456",
  "level": "ERROR",
  "logger": "adeline.inference.pipeline",
  "message": "Error pausing pipeline: Pipeline already paused",
  "trace_id": "cmd-pause-abc123",
  "component": "inference_pipeline",
  "error_type": "InvalidStateError"
}
```

---

## Performance Analysis Queries

### Calculate Average FPS

```bash
# Average FPS across all metrics logs
jq -s 'map(select(.metrics.fps)) | map(.metrics.fps) | add/length' logs/adeline.log

# Average FPS in last hour (requires timestamp filtering)
jq -s '
  map(select(.metrics.fps and .timestamp > "2025-10-22T15:00:00")) |
  map(.metrics.fps) |
  add/length
' logs/adeline.log
```

**Use Case:** Performance trending, capacity planning, regression detection.

**Output Example:**

```
30.47
```

---

### Find FPS Outliers (Low Performance)

```bash
# FPS below 20 (performance degradation)
jq 'select(.metrics.fps and .metrics.fps < 20)' logs/adeline.log

# FPS below 20 with context
jq 'select(.metrics.fps and .metrics.fps < 20) | {
  timestamp,
  fps: .metrics.fps,
  latency: .metrics.latency_ms,
  frames_processed: .metrics.frames_processed
}' logs/adeline.log
```

**Use Case:** Detect performance degradation, identify resource bottlenecks.

---

### Latency Distribution

```bash
# Min/Max/Avg latency
jq -s '
  map(select(.metrics.latency_ms)) |
  {
    min: (map(.metrics.latency_ms) | min),
    max: (map(.metrics.latency_ms) | max),
    avg: (map(.metrics.latency_ms) | add/length)
  }
' logs/adeline.log
```

**Output Example:**

```json
{
  "min": 10.5,
  "max": 45.2,
  "avg": 15.8
}
```

**Use Case:** Latency SLA monitoring, performance optimization.

---

### FPS Time Series

```bash
# FPS over time (timestamp + fps)
jq -r 'select(.metrics.fps) | "\(.timestamp) \(.metrics.fps)"' logs/adeline.log

# Export to CSV for graphing
jq -r 'select(.metrics.fps) | [.timestamp, .metrics.fps] | @csv' logs/adeline.log > fps_timeseries.csv
```

**Use Case:** Visualize performance trends, correlate FPS drops with external events.

---

## Stabilization Analysis Queries

### Stabilization Effectiveness (Raw vs Stabilized Ratio)

```bash
# Extract raw vs stabilized counts
jq 'select(.stabilization) | {
  timestamp,
  raw: .stabilization.raw_count,
  stabilized: .stabilization.stabilized_count,
  ratio: (.stabilization.stabilized_count / .stabilization.raw_count)
}' logs/adeline.log
```

**Use Case:** Tune stabilization thresholds, measure filtering effectiveness.

**Output Example:**

```json
{
  "timestamp": "2025-10-22T16:30:45.456789",
  "raw": 12,
  "stabilized": 8,
  "ratio": 0.6666666666666666
}
```

---

### Active Tracks Over Time

```bash
# Track count time series
jq -r 'select(.stabilization.active_tracks) |
  "\(.timestamp) \(.stabilization.active_tracks)"' logs/adeline.log

# Average active tracks
jq -s '
  map(select(.stabilization.active_tracks)) |
  map(.stabilization.active_tracks) |
  add/length
' logs/adeline.log
```

**Use Case:** Monitor multi-person tracking, detect room occupancy patterns.

---

### Track Lifecycle Events

```bash
# All track creation events
jq 'select(.event_type == "track_created")' logs/adeline.log

# All track removal events
jq 'select(.event_type == "track_lost")' logs/adeline.log

# Track lifetime (creation â†’ removal)
jq -s '
  group_by(.track_id) |
  map({
    track_id: .[0].track_id,
    created: (map(select(.event_type == "track_created")) | .[0].timestamp),
    lost: (map(select(.event_type == "track_lost")) | .[0].timestamp)
  })
' logs/adeline.log
```

**Use Case:** Analyze track stability, tune hysteresis thresholds.

---

## Error Analysis Queries

### Errors by Component

```bash
# Count errors by component
jq -s '
  map(select(.level == "ERROR")) |
  group_by(.component) |
  map({component: .[0].component, count: length})
' logs/adeline.log
```

**Output Example:**

```json
[
  {"component": "control_plane", "count": 3},
  {"component": "data_plane", "count": 7},
  {"component": "stabilization", "count": 1}
]
```

**Use Case:** Identify which component is generating most errors.

---

### Errors by Type

```bash
# Count errors by exception type
jq -s '
  map(select(.error_type)) |
  group_by(.error_type) |
  map({error_type: .[0].error_type, count: length})
' logs/adeline.log
```

**Output Example:**

```json
[
  {"error_type": "ConnectionRefusedError", "count": 5},
  {"error_type": "TimeoutError", "count": 2},
  {"error_type": "JSONDecodeError", "count": 1}
]
```

**Use Case:** Prioritize error fixes based on frequency.

---

### Recent Errors with Context

```bash
# Last 10 errors with full context
jq 'select(.level == "ERROR")' logs/adeline.log | tail -n 10 | jq .

# Errors with specific fields
jq 'select(.level == "ERROR") | {
  timestamp,
  component,
  error_type,
  error_message,
  trace_id
}' logs/adeline.log
```

**Use Case:** Rapid error triage, debugging latest failures.

---

## MQTT Analysis Queries

### Command Frequency

```bash
# Count commands by type
jq -s '
  map(select(.command)) |
  group_by(.command) |
  map({command: .[0].command, count: length})
' logs/adeline.log
```

**Output Example:**

```json
[
  {"command": "pause", "count": 12},
  {"command": "resume", "count": 10},
  {"command": "status", "count": 45},
  {"command": "stop", "count": 2}
]
```

**Use Case:** Understand command usage patterns, optimize command handling.

---

### Publish Success Rate

```bash
# Success rate for MQTT publishes
jq -s '
  map(select(.mqtt_topic and .success != null)) |
  {
    total: length,
    successful: (map(select(.success == true)) | length),
    failed: (map(select(.success == false)) | length),
    success_rate: ((map(select(.success == true)) | length) / length * 100)
  }
' logs/adeline.log
```

**Output Example:**

```json
{
  "total": 1000,
  "successful": 997,
  "failed": 3,
  "success_rate": 99.7
}
```

**Use Case:** Monitor MQTT broker reliability, detect network issues.

---

### Bandwidth Analysis

```bash
# Total bytes published by topic
jq -s '
  map(select(.payload_size_bytes)) |
  group_by(.mqtt_topic) |
  map({
    topic: .[0].mqtt_topic,
    total_bytes: (map(.payload_size_bytes) | add),
    avg_bytes: (map(.payload_size_bytes) | add/length),
    message_count: length
  })
' logs/adeline.log
```

**Output Example:**

```json
[
  {
    "topic": "inference/data/detections",
    "total_bytes": 102400,
    "avg_bytes": 1024,
    "message_count": 100
  },
  {
    "topic": "inference/data/metrics",
    "total_bytes": 5120,
    "avg_bytes": 512,
    "message_count": 10
  }
]
```

**Use Case:** Bandwidth optimization, topic sizing analysis.

---

## Advanced Queries

### Combine Multiple Filters

```bash
# Errors in Control Plane with trace_id
jq 'select(.level == "ERROR" and .component == "control_plane" and .trace_id != null)' logs/adeline.log

# High latency metrics (>50ms) with low FPS (<25)
jq 'select(.metrics.latency_ms > 50 and .metrics.fps < 25)' logs/adeline.log
```

**Use Case:** Complex filtering for specific debugging scenarios.

---

### Time Window Queries

```bash
# Logs between two timestamps
jq 'select(.timestamp >= "2025-10-22T16:00:00" and .timestamp <= "2025-10-22T17:00:00")' logs/adeline.log

# Last hour (requires current timestamp)
NOW=$(date -u +"%Y-%m-%dT%H:%M:%S")
HOUR_AGO=$(date -u -d '1 hour ago' +"%Y-%m-%dT%H:%M:%S")
jq "select(.timestamp >= \"$HOUR_AGO\" and .timestamp <= \"$NOW\")" logs/adeline.log
```

**Use Case:** Incident investigation, time-based correlation.

---

### JSON to CSV for Spreadsheet Analysis

```bash
# FPS metrics to CSV
jq -r 'select(.metrics.fps) |
  [.timestamp, .metrics.fps, .metrics.latency_ms] |
  @csv' logs/adeline.log > metrics.csv

# Errors to CSV
jq -r 'select(.level == "ERROR") |
  [.timestamp, .component, .error_type, .error_message] |
  @csv' logs/adeline.log > errors.csv
```

**Use Case:** Import into Excel/Google Sheets for visualization.

---

## Live Monitoring with `watch`

```bash
# Live FPS monitoring
watch -n 2 'tail -n 100 logs/adeline.log | jq -s "
  map(select(.metrics.fps)) |
  map(.metrics.fps) |
  add/length
"'

# Live error count
watch -n 5 'jq -r "select(.level == \"ERROR\")" logs/adeline.log | wc -l'

# Live active tracks
watch -n 1 'tail -n 50 logs/adeline.log | jq -r "
  select(.stabilization.active_tracks) |
  .stabilization.active_tracks
" | tail -n 1'
```

**Use Case:** Real-time monitoring dashboards, live debugging.

---

## Query Cookbook Summary

|Use Case|Query Pattern|
|---|---|
|**Debugging**|Filter by component/level/trace_id|
|**Performance**|Aggregate FPS, analyze latency distribution|
|**Tracing**|Correlate commands end-to-end via trace_id|
|**Errors**|Group by component/type, find recent failures|
|**Stabilization**|Track lifecycle, raw vs stabilized ratio|
|**MQTT**|Command frequency, publish success rate, bandwidth|
|**Time-Based**|Filter by timestamp windows|
|**Export**|Convert to CSV for external analysis|

---

## Tools and Tips

### Useful `jq` Options

```bash
# Compact output (no pretty-print)
jq -c 'select(.level == "ERROR")' logs/adeline.log

# Raw output (no JSON quoting)
jq -r '.message' logs/adeline.log

# Slurp mode (read entire file as array)
jq -s 'length' logs/adeline.log  # Count total logs

# Color output (default in terminal)
jq . logs/adeline.log | less -R  # Preserve colors in pager
```

### Combining with Other Tools

```bash
# Grep before jq for performance (filter lines first)
grep '"control_plane"' logs/adeline.log | jq 'select(.level == "ERROR")'

# Count specific events
jq 'select(.command == "pause")' logs/adeline.log | wc -l

# Sort by timestamp
jq -s 'sort_by(.timestamp)' logs/adeline.log
```

### Performance Tips

1. **Use grep first for large files** - Filter lines before piping to `jq`
2. **Use `-c` for compact output** - Faster than pretty-printing
3. **Limit input with `head`/`tail`** - Process only relevant portion
4. **Use `--stream` for very large files** - Memory-efficient parsing

**Sources:** [adeline/CLAUDE.md201-208](https://github.com/acare7/kata-inference-251021-clean4/blob/master/adeline/CLAUDE.md#L201-L208)

---

## Summary

Structured JSON logging enables powerful production queries:

1. **No Parsing Required** - Direct field access via `jq`
2. **Trace Correlation** - Follow commands end-to-end via `trace_id`
3. **Performance Analysis** - Aggregate FPS, latency, throughput
4. **Error Analysis** - Group by component/type, find root causes
5. **Real-Time Monitoring** - Live dashboards with `watch`

**Design Philosophy:** Queryability by design - upfront structure investment pays off in production observability.
