"""
Adaptive ROI Inference Pipeline Module
=======================================

Bounded Context: Inference Orchestration (flow end-to-end)

This module orchestrates the adaptive ROI inference pipeline:
- Frame transformations: crop, coordinate mapping
- Inference orchestration: ROI ‚Üí crop ‚Üí infer ‚Üí transform ‚Üí metrics
- Sink integration: ROI update callbacks
- Handler interface: Mutable wrapper for InferencePipeline integration

Design:
- Orquestration layer (delegates to geometry + state modules)
- NumPy-optimized operations (vectorized transforms)
- Zero-copy crop (NumPy views)
- Toggle support (enable/disable via MQTT)

Performance optimizations:
- NumPy views for crop (zero-copy)
- Vectorized coordinate transforms (~20x faster than loops)
- Optional metrics computation (disable in production for better perf)
"""

from typing import List, Dict, Optional, Tuple, Union
import logging

import cv2
import numpy as np
import supervision as sv
from inference.core.interfaces.camera.entities import VideoFrame

from .geometry import ROIBox
from .state import ROIState
from ...handlers.base import BaseInferenceHandler


logger = logging.getLogger(__name__)

# Track sources que ya mostraron log de resize (para no spamear logs)
_resize_logged_sources = set()


# ============================================================================
# Optimized Crop Functions
# ============================================================================

def crop_frame_if_roi(
    video_frame: VideoFrame,
    roi: Optional[ROIBox],
    model_size: Optional[int] = None,
    resize_to_model: bool = False,
) -> Tuple[VideoFrame, Optional[Tuple[int, int]]]:
    """
    Crop eficiente usando numpy view (sin copia), con resize opcional.

    Performance: NumPy slicing crea VIEW, no copia datos - muy eficiente.

    Resize Strategy (resize_to_model=True):
    - Si ROI < model_size: Resize (zoom) a model_size√ómodel_size
    - Si ROI >= model_size: Sin resize (model hace padding/resize autom√°tico)
    - Trade-off: Mejor detecci√≥n peque√±a vs. p√©rdida de escala original

    Args:
        video_frame: Frame original
        roi: ROI a aplicar, o None para frame completo
        model_size: Tama√±o del modelo (ej: 320, 640) para resize
        resize_to_model: Si True, resize ROI a model_size (zoom). Si False, padding con negro.

    Returns:
        (cropped_frame, (offset_x, offset_y)) o (original_frame, None)
    """
    if roi is None:
        return video_frame, None

    # Numpy slicing crea VIEW (no copia) - muy eficiente
    cropped_image = video_frame.image[roi.y1:roi.y2, roi.x1:roi.x2]

    # Validar crop no vac√≠o
    if cropped_image.size == 0:
        logger.warning(f"Empty crop for ROI {roi}, using full frame")
        return video_frame, None

    # Resize condicional (solo si resize_to_model=True y ROI < model_size)
    if resize_to_model and model_size is not None:
        roi_size = max(cropped_image.shape[0], cropped_image.shape[1])

        if roi_size < model_size:
            # Zoom: resize a model_size√ómodel_size
            # INTER_LINEAR es buen balance entre calidad y performance
            original_size = roi_size
            cropped_image = cv2.resize(
                cropped_image,
                (model_size, model_size),
                interpolation=cv2.INTER_LINEAR
            )

            # Log solo la primera vez por source (evitar spam en logs)
            source_id = video_frame.source_id
            if source_id not in _resize_logged_sources:
                logger.info(
                    f"üîç Zoom aplicado (source {source_id}): "
                    f"ROI {original_size}√ó{original_size} ‚Üí {model_size}√ó{model_size}"
                )
                _resize_logged_sources.add(source_id)

    # Nuevo VideoFrame (solo metadata nueva, imagen es view o resized)
    cropped_frame = VideoFrame(
        image=cropped_image,
        frame_id=video_frame.frame_id,
        frame_timestamp=video_frame.frame_timestamp,
        source_id=video_frame.source_id,
    )

    return cropped_frame, (roi.x1, roi.y1)


def transform_predictions_vectorized(
    predictions: dict,
    crop_offset: Optional[Tuple[int, int]],
) -> dict:
    """
    Transforma coordenadas usando operaciones vectorizadas.

    Performance: ~20x m√°s r√°pido que loop en Python puro.

    Args:
        predictions: Dict con predictions de modelo Roboflow
        crop_offset: (x_offset, y_offset) del crop, o None si no hay crop

    Returns:
        predictions con coordenadas transformadas al frame original
    """
    if crop_offset is None or 'predictions' not in predictions:
        return predictions

    x_offset, y_offset = crop_offset
    detections_list = predictions['predictions']

    if not detections_list:
        return predictions

    # Extraer coordenadas en arrays numpy (m√°s eficiente que loop)
    xs = np.array([d['x'] for d in detections_list])
    ys = np.array([d['y'] for d in detections_list])

    # Operaci√≥n vectorizada: suma broadcast
    xs += x_offset
    ys += y_offset

    # Actualizar in-place (evita crear nuevo dict)
    for i, det in enumerate(detections_list):
        det['x'] = float(xs[i])  # numpy float64 -> Python float
        det['y'] = float(ys[i])

    return predictions


def convert_predictions_to_sv_detections(predictions: dict) -> sv.Detections:
    """
    Convierte predictions dict a sv.Detections para usar supervision utils.

    Optimizado con operaciones vectorizadas.

    Args:
        predictions: Dict con formato Roboflow standard

    Returns:
        sv.Detections object
    """
    detections_list = predictions.get('predictions', [])

    if not detections_list:
        return sv.Detections.empty()

    # Operaciones vectorizadas para construir xyxy
    n = len(detections_list)
    xyxy = np.zeros((n, 4), dtype=np.float32)

    # Extraer en paralelo (list comprehension m√°s r√°pida que loop)
    xs = np.array([d['x'] for d in detections_list], dtype=np.float32)
    ys = np.array([d['y'] for d in detections_list], dtype=np.float32)
    ws = np.array([d['width'] for d in detections_list], dtype=np.float32)
    hs = np.array([d['height'] for d in detections_list], dtype=np.float32)

    # Operaciones vectorizadas: (x,y,w,h) -> (x1,y1,x2,y2)
    xyxy[:, 0] = xs - ws / 2  # x1
    xyxy[:, 1] = ys - hs / 2  # y1
    xyxy[:, 2] = xs + ws / 2  # x2
    xyxy[:, 3] = ys + hs / 2  # y2

    confidence = np.array([d.get('confidence', 1.0) for d in detections_list], dtype=np.float32)
    class_id = np.array([d.get('class_id', 0) for d in detections_list], dtype=int)

    return sv.Detections(
        xyxy=xyxy,
        confidence=confidence,
        class_id=class_id,
    )


# ============================================================================
# Main Inference Pipeline
# ============================================================================

def adaptive_roi_inference(
    video_frames: List[VideoFrame],
    model,
    inference_config,
    roi_state: ROIState,
    enable_crop: bool = True,
    process_frame_fn=None,
    show_statistics: bool = True,
) -> List[dict]:
    """
    Pipeline de inferencia con crop adaptativo optimizado.

    Flow por frame:
    1. Get ROI del estado compartido
    2. Crop eficiente (numpy view - zero copy)
    3. Inferencia con modelo (Roboflow o local ONNX)
    4. Transform coords de vuelta al frame original (vectorizado)
    5. Agregar m√©tricas de ROI/performance (opcional, seg√∫n show_statistics)

    Performance optimizations:
    - NumPy views para crop (no copia)
    - Operaciones vectorizadas para coordenadas
    - Supervision utilities para bbox ops
    - Metrics computation opcional (desactivar en producci√≥n para mejor performance)

    Args:
        video_frames: Batch de frames a procesar
        model: Modelo de inferencia (Roboflow o LocalONNXModel)
        inference_config: Configuraci√≥n del modelo
        roi_state: Estado compartido de ROI
        enable_crop: Flag para habilitar/deshabilitar crop
        process_frame_fn: Funci√≥n para procesar frames (auto-detectada si None)
        show_statistics: Si calcular y publicar m√©tricas de performance (default: True)

    Returns:
        Lista de predictions (formato Roboflow standard) con m√©tricas de ROI (si show_statistics=True)
    """
    # Auto-detectar funci√≥n de procesamiento si no se provee
    if process_frame_fn is None:
        from inference.core.interfaces.stream.model_handlers.roboflow_models import (
            default_process_frame,
        )
        process_frame_fn = default_process_frame

    results = []

    for video_frame in video_frames:
        # 1. Get ROI del estado
        roi = None
        if enable_crop:
            # Soporte para ROIState (adaptive) y FixedROIState (fixed)
            # FixedROIState necesita frame_shape para convertir coords normalizadas
            frame_shape = video_frame.image.shape[:2]

            # Intentar llamar con frame_shape (FixedROIState)
            try:
                roi = roi_state.get_roi(source_id=video_frame.source_id, frame_shape=frame_shape)
            except TypeError:
                # Fallback: ROIState (adaptive) solo necesita source_id
                roi = roi_state.get_roi(source_id=video_frame.source_id)

        # 2. Crop eficiente (numpy view) + resize opcional
        model_size = getattr(roi_state, '_imgsz', None)
        resize_to_model = getattr(roi_state, 'resize_to_model', False)
        frame_to_infer, crop_offset = crop_frame_if_roi(
            video_frame,
            roi,
            model_size=model_size,
            resize_to_model=resize_to_model,
        )

        # 3. Inferencia (funci√≥n configurable: Roboflow o local ONNX)
        prediction = process_frame_fn(
            [frame_to_infer],
            model=model,
            inference_config=inference_config
        )[0]

        # 4. Transform coords (vectorizado)
        prediction = transform_predictions_vectorized(prediction, crop_offset)

        # 5. Metadata + m√©tricas para monitoring/debugging (solo si show_statistics=True)
        if show_statistics:
            frame_shape = video_frame.image.shape[:2]
            crop_metadata = {
                'enabled': enable_crop,
                'crop_applied': crop_offset is not None,
                'crop_offset': crop_offset,
            }

            if roi:
                # M√©tricas del ROI
                crop_metadata['roi'] = {
                    'x1': roi.x1,
                    'y1': roi.y1,
                    'x2': roi.x2,
                    'y2': roi.y2,
                    'width': roi.width,
                    'height': roi.height,
                    'area': roi.area,
                    'is_square': roi.is_square,
                }

                # Performance metrics
                # Soporte para ROIState (adaptive) y FixedROIState (fixed)
                imgsz = getattr(roi_state, '_imgsz', None)
                crop_metadata['performance'] = {
                    'imgsz': imgsz,
                    'size_multiple': roi.get_size_multiple(imgsz) if imgsz else None,
                    'crop_ratio': roi.get_crop_ratio(frame_shape),
                    'pixel_reduction': 1.0 - roi.get_crop_ratio(frame_shape),
                    'frame_size': {'height': frame_shape[0], 'width': frame_shape[1]},
                }
            else:
                # Full frame (no crop)
                imgsz = getattr(roi_state, '_imgsz', None)
                crop_metadata['roi'] = None
                crop_metadata['performance'] = {
                    'imgsz': imgsz,
                    'size_multiple': 0.0,  # No crop
                    'crop_ratio': 1.0,  # Full frame
                    'pixel_reduction': 0.0,  # No reduction
                    'frame_size': {'height': frame_shape[0], 'width': frame_shape[1]},
                }

            prediction['__crop_metadata__'] = crop_metadata

        results.append(prediction)

    return results


# ============================================================================
# Sink for ROI Update
# ============================================================================

def roi_update_sink(
    predictions: Union[dict, List[Optional[dict]]],
    video_frames: Union[VideoFrame, List[Optional[VideoFrame]]],
    roi_state: ROIState,
):
    """
    Sink que actualiza ROI state basado en detecciones.

    Este sink NO hace I/O, solo actualiza estado compartido.
    Debe combinarse con otros sinks usando multi_sink.

    Args:
        predictions: Predictions del modelo
        video_frames: Frames correspondientes
        roi_state: Estado compartido de ROI
    """
    # Normalizar a listas (para compatibilidad con SinkMode.ADAPTIVE)
    if not isinstance(predictions, list):
        predictions = [predictions]
        video_frames = [video_frames]

    for prediction, video_frame in zip(predictions, video_frames):
        if prediction is None or video_frame is None:
            continue

        # Convertir a sv.Detections (operaciones vectorizadas)
        detections = convert_predictions_to_sv_detections(prediction)

        # Actualizar ROI (operaciones numpy optimizadas)
        roi_state.update_from_detections(
            source_id=video_frame.source_id,
            detections=detections,
            frame_shape=video_frame.image.shape[:2],
        )


# ============================================================================
# Adaptive Inference Handler (Mutable for MQTT Toggle)
# ============================================================================

class AdaptiveInferenceHandler(BaseInferenceHandler):
    """
    Wrapper callable para adaptive_roi_inference que permite toggle din√°mico.

    Este wrapper es necesario porque InferencePipeline.init_with_custom_logic()
    espera un callable, pero necesitamos poder cambiar el flag 'enabled' en runtime
    (v√≠a MQTT command).

    Hereda de BaseInferenceHandler para garantizar interface consistente.

    Properties:
    - enabled: Si ROI adaptativo est√° habilitado (getter/setter)
    - supports_toggle: True (soporta enable/disable din√°mico)

    Methods:
    - enable(): Habilita ROI adaptativo
    - disable(): Deshabilita ROI y resetea state

    Usage:
        handler = AdaptiveInferenceHandler(model, config, roi_state)
        pipeline = InferencePipeline.init_with_custom_logic(
            on_video_frame=handler,  # callable
            ...
        )

        # Luego, desde MQTT callback:
        handler.disable()  # Disable crop din√°micamente
        handler.enable()   # Re-enable
    """

    def __init__(
        self,
        model,
        inference_config,
        roi_state: ROIState,
        process_frame_fn=None,
        show_statistics: bool = True,
    ):
        self.model = model
        self.inference_config = inference_config
        self.roi_state = roi_state
        self.process_frame_fn = process_frame_fn
        self._enabled = True  # Internal mutable flag
        self.show_statistics = show_statistics  # Performance optimization flag

    @property
    def enabled(self) -> bool:
        """Si adaptive ROI est√° habilitado."""
        return self._enabled

    @property
    def supports_toggle(self) -> bool:
        """Adaptive ROI soporta toggle din√°mico."""
        return True

    def enable(self):
        """Habilita adaptive ROI."""
        self._enabled = True
        logger.info("‚úÖ Adaptive ROI enabled")

    def disable(self):
        """Deshabilita adaptive ROI y resetea state a full frame."""
        self._enabled = False
        self.roi_state.reset()
        logger.info("üî≤ Adaptive ROI disabled (reset to full frame)")

    def __call__(self, video_frames: List[VideoFrame]) -> List[dict]:
        """Callable interface para InferencePipeline"""
        return adaptive_roi_inference(
            video_frames=video_frames,
            model=self.model,
            inference_config=self.inference_config,
            roi_state=self.roi_state,
            enable_crop=self._enabled,  # Lee el flag interno
            process_frame_fn=self.process_frame_fn,
            show_statistics=self.show_statistics,  # Control metrics computation
        )
