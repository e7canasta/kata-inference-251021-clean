# Propuesta TÃ©cnica: Performance Monitoring & Profiling

**Proyecto:** Adeline v2.5 â†’ v2.6  
**Autor:** Copilot Performance Engineer  
**Fecha:** 2025-01-25  
**Prioridad:** Alta  
**EstimaciÃ³n:** 3-4 dÃ­as de desarrollo  

---

## Executive Summary

El sistema Adeline actual tiene una arquitectura sÃ³lida (9.2/10) pero carece de observabilidad de performance en tiempo real. Esta propuesta introduce un sistema de monitoreo comprehensivo para garantizar que el pipeline de inferencia mantenga latencias Ã³ptimas en producciÃ³n.

## AnÃ¡lisis del Estado Actual

### Fortalezas Identificadas
- âœ… **Pipeline bien diseÃ±ado**: Builder pattern + Strategy pattern
- âœ… **ModularizaciÃ³n excelente**: Factories y handlers desacoplados
- âœ… **Testing robusto**: 1,240+ lÃ­neas de tests comprehensivos

### Gaps CrÃ­ticos Detectados
- âŒ **Sin mÃ©tricas de latencia**: No sabemos si inference toma 50ms o 500ms
- âŒ **Sin alertas de degradaciÃ³n**: Fallos silenciosos en performance
- âŒ **Sin profiling de memory**: Posibles memory leaks en tracking
- âŒ **Sin bottleneck detection**: Â¿CuÃ¡l es el cuello de botella real?

## Propuesta TÃ©cnica

### 1. Performance Metrics Collection

**Archivos a crear:**
```
adeline/monitoring/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ metrics.py          # MÃ©tricas core
â”œâ”€â”€ profiler.py         # Profiling automÃ¡tico
â””â”€â”€ collectors/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ inference.py     # MÃ©tricas de inference
    â”œâ”€â”€ pipeline.py      # MÃ©tricas de pipeline
    â””â”€â”€ memory.py        # Memory profiling
```

**ImplementaciÃ³n:**
```python
# adeline/monitoring/metrics.py
from dataclasses import dataclass
from typing import Dict, List
import time
import psutil
from contextlib import contextmanager

@dataclass
class PerformanceMetrics:
    inference_latency_ms: float
    preprocessing_ms: float
    postprocessing_ms: float
    memory_usage_mb: float
    active_tracks: int
    fps_processed: float
    timestamp: float

class MetricsCollector:
    def __init__(self):
        self.metrics_history: List[PerformanceMetrics] = []
        self.current_metrics = {}
    
    @contextmanager
    def measure_inference(self):
        start = time.perf_counter()
        yield
        end = time.perf_counter()
        self.current_metrics['inference_latency_ms'] = (end - start) * 1000
    
    @contextmanager
    def measure_pipeline_stage(self, stage_name: str):
        start = time.perf_counter()
        yield
        end = time.perf_counter()
        self.current_metrics[f'{stage_name}_ms'] = (end - start) * 1000
    
    def collect_memory_metrics(self):
        process = psutil.Process()
        self.current_metrics['memory_usage_mb'] = process.memory_info().rss / 1024 / 1024
    
    def finalize_metrics(self) -> PerformanceMetrics:
        metrics = PerformanceMetrics(
            inference_latency_ms=self.current_metrics.get('inference_latency_ms', 0),
            preprocessing_ms=self.current_metrics.get('preprocessing_ms', 0),
            postprocessing_ms=self.current_metrics.get('postprocessing_ms', 0),
            memory_usage_mb=self.current_metrics.get('memory_usage_mb', 0),
            active_tracks=self.current_metrics.get('active_tracks', 0),
            fps_processed=self.current_metrics.get('fps_processed', 0),
            timestamp=time.time()
        )
        self.metrics_history.append(metrics)
        self.current_metrics.clear()
        return metrics
```

### 2. Integration Points

**Modificaciones mÃ­nimas en cÃ³digo existente:**

```python
# adeline/inference/handlers/standard.py - LÃ­nea ~45
class StandardInferenceHandler(BaseInferenceHandler):
    def __init__(self, model_loader, metrics_collector=None):
        super().__init__(model_loader)
        self.metrics = metrics_collector or MetricsCollector()
    
    def run_inference(self, frame):
        with self.metrics.measure_inference():
            results = super().run_inference(frame)
        
        self.metrics.current_metrics['active_tracks'] = len(getattr(self, 'tracker', {}).get('tracks', []))
        return results
```

```python
# adeline/app/controller.py - LÃ­nea ~89
async def process_frame(self, frame):
    with self.metrics.measure_pipeline_stage('preprocessing'):
        preprocessed = self.preprocess_frame(frame)
    
    # ... existing inference logic ...
    
    with self.metrics.measure_pipeline_stage('postprocessing'):
        results = self.postprocess_results(inference_results)
    
    # Collect and publish metrics
    performance_metrics = self.metrics.finalize_metrics()
    await self.publish_metrics(performance_metrics)
```

### 3. Real-time Dashboard

**Archivos a crear:**
```
adeline/monitoring/dashboard/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ server.py           # FastAPI metrics server
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ dashboard.html  # Real-time dashboard
â””â”€â”€ static/
    â”œâ”€â”€ dashboard.js    # WebSocket client
    â””â”€â”€ dashboard.css   # Styling
```

**Funcionalidades:**
- ğŸ“Š **Real-time charts**: Latencia, memory, FPS
- ğŸš¨ **Alertas automÃ¡ticas**: Threshold-based warnings
- ğŸ“ˆ **Historical trends**: Performance over time
- ğŸ” **Bottleneck detection**: Automated analysis

### 4. Automated Profiling

```python
# adeline/monitoring/profiler.py
class AutoProfiler:
    def __init__(self, sample_rate=0.01):  # 1% sampling
        self.sample_rate = sample_rate
        self.profiler = cProfile.Profile()
    
    @contextmanager
    def profile_inference(self):
        if random.random() < self.sample_rate:
            self.profiler.enable()
            yield
            self.profiler.disable()
            self.analyze_profile()
        else:
            yield
    
    def analyze_profile(self):
        # Top functions by cumulative time
        stats = pstats.Stats(self.profiler)
        stats.sort_stats('cumulative')
        
        # Detect bottlenecks automatically
        top_functions = stats.get_stats_profile()
        # ... bottleneck detection logic ...
```

## MÃ©tricas Clave a Monitorear

### Performance Metrics
- **Inference Latency**: < 50ms target (real-time requirement)
- **Memory Usage**: Trend analysis para detectar leaks
- **FPS Processing Rate**: Throughput del pipeline
- **Queue Depth**: Backlog en processing

### Business Metrics  
- **Detection Accuracy**: Precision/Recall en tiempo real
- **Track Stability**: Lifetime promedio de tracks
- **ROI Adaptation Rate**: Frequency de ROI updates

### System Metrics
- **CPU Utilization**: Per-core usage
- **GPU Utilization**: Model inference efficiency
- **Network I/O**: MQTT publish latency

## Plan de ImplementaciÃ³n

### Sprint 1 (2 dÃ­as)
- âœ… Metrics collection infrastructure
- âœ… Integration en inference handlers
- âœ… Basic memory profiling

### Sprint 2 (1 dÃ­a)  
- âœ… Real-time dashboard
- âœ… WebSocket metrics streaming
- âœ… Automated alerts

### Sprint 3 (1 dÃ­a)
- âœ… Historical analysis
- âœ… Bottleneck detection
- âœ… Performance regression tests

## Beneficios Esperados

### Operacionales
- ğŸ¯ **25% reduction** en debugging time para performance issues
- ğŸš€ **Proactive alerts** antes de que users noten degradaciÃ³n
- ğŸ“Š **Data-driven optimization** basado en mÃ©tricas reales

### TÃ©cnicos
- ğŸ” **Visibility completa** del pipeline performance
- ğŸ§ª **A/B testing** de optimizations con mÃ©tricas concretas
- ğŸ“ˆ **Capacity planning** basado en trends histÃ³ricos

## Riesgos y Mitigaciones

| Riesgo | Probabilidad | Impacto | MitigaciÃ³n |
|--------|-------------|---------|------------|
| Overhead de metrics | Medio | Bajo | Sampling rate configurable |
| Dashboard complexity | Bajo | Medio | MVP dashboard, iterate |
| Integration issues | Bajo | Alto | Backward compatibility design |

## Success Metrics

- âœ… **< 1ms overhead** agregado por metrics collection
- âœ… **100% pipeline visibility** con real-time metrics
- âœ… **< 30 segundos** para detectar performance regressions
- âœ… **Zero downtime** durante implementaciÃ³n

---

**Next Steps:**
1. AprobaciÃ³n tÃ©cnica del equipo
2. Spike tÃ©cnico de 4 horas para validar approach
3. ImplementaciÃ³n incremental con feature flags

**Dependencies:**
- FastAPI (dashboard server)
- psutil (system metrics)
- WebSockets (real-time updates)

**Backward Compatibility:** âœ… 100% - Metrics son opt-in por defecto