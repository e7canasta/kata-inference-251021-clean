# Inference Pipeline

Relevant source files

- [README.md](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/README.md)
- [adeline/CLAUDE.md](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/CLAUDE.md)
- [adeline/app/controller.py](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py)

## Purpose and Scope

This document describes the **Inference Pipeline** component, which performs YOLO-based object detection on video frames from RTSP streams. The pipeline processes frames through configurable ROI (Region of Interest) strategies, applies detection stabilization, and outputs results to multiple sinks.

This page covers:

- Pipeline initialization and configuration
- Two operational modes: standard and custom logic
- Frame processing flow and model inference
- Model management (local ONNX vs Roboflow API)
- Integration points with ROI and stabilization components

For orchestration and lifecycle management, see [Pipeline Controller](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/5.1-pipeline-controller). For ROI processing details, see [ROI Strategies](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/5.5-roi-strategies). For stabilization details, see [Detection Stabilization](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/5.6-detection-stabilization). For output publishing, see [Data Plane](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/5.4-data-plane).

---

## Pipeline Architecture Overview

The Inference Pipeline operates in two distinct modes depending on whether ROI strategies are enabled. Both modes use the `InferencePipeline` class from the Roboflow `inference` library, but differ in how frame processing occurs.

**Diagram: Pipeline Mode Selection and Initialization**

Sources: [adeline/app/controller.py130-276](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L130-L276)

---

## Standard Pipeline Mode

The standard mode uses the Roboflow `InferencePipeline.init()` method with built-in inference logic. This mode is appropriate when no custom preprocessing (ROI cropping) is needed.

### Configuration

Standard mode is activated when `roi_strategy.mode` is set to `"none"` in configuration:

```
roi_strategy:
  mode: "none"
```

### Initialization

The pipeline is created with minimal configuration:

```
self.pipeline = InferencePipeline.init(
    max_fps=self.config.MAX_FPS,
    model_id=self.config.MODEL_ID,
    video_reference=self.config.RTSP_URL,
    on_prediction=on_prediction,
    api_key=self.config.API_KEY,
    watchdog=self.watchdog,
    status_update_handlers=[self._status_update_handler],
)
```

|Parameter|Description|Config Source|
|---|---|---|
|`max_fps`|Target frame rate for processing|`pipeline.max_fps`|
|`model_id`|Roboflow model identifier|`models.model_id`|
|`video_reference`|RTSP stream URL|`pipeline.rtsp_url`|
|`on_prediction`|Sink function for detection results|Composed from config|
|`api_key`|Roboflow API key|`.env` → `ROBOFLOW_API_KEY`|
|`watchdog`|Metrics monitoring instance|Created in controller|
|`status_update_handlers`|Status callback list|Internal handler|

### Processing Flow

In standard mode, the pipeline:

1. Reads frames from RTSP stream
2. Runs inference using Roboflow cloud API
3. Invokes `on_prediction` with detection results
4. Publishes metrics via watchdog

Sources: [adeline/app/controller.py247-276](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L247-L276) [adeline/config.py](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py)

---

## Custom Logic Pipeline Mode

Custom logic mode uses `InferencePipeline.init_with_custom_logic()` to inject custom frame preprocessing before inference. This enables ROI strategies (adaptive/fixed) to crop frames dynamically or statically.

### Configuration

Custom mode is activated when `roi_strategy.mode` is set to `"adaptive"` or `"fixed"`:

```
roi_strategy:
  mode: "adaptive"  # or "fixed"
```

### Model Loading

Unlike standard mode, custom mode explicitly loads models:

**Diagram: Model Loading Factory Pattern**

The factory function `get_model_from_config()` abstracts model source:

- **Local ONNX**: Loads from file system at `models.local_model_path`
- **Roboflow API**: Downloads model using `models.model_id` and API key

Sources: [adeline/inference/models.py](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/inference/models.py) [adeline/app/controller.py171-189](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L171-L189)

### Inference Handler Creation

Custom mode requires an inference handler that wraps the model and implements custom preprocessing:

**Diagram: Inference Handler Factory Pattern**

The handler receives each video frame and:

1. Extracts ROI coordinates from state object
2. Crops frame to ROI (if enabled)
3. Runs inference on cropped region
4. Transforms detection coordinates back to full frame space

|Handler Class|ROI Behavior|State Type|
|---|---|---|
|`AdaptiveInferenceHandler`|Dynamic crop based on previous detections|`AdaptiveROIState`|
|`FixedROIInferenceHandler`|Static crop at configured coordinates|`FixedROIState`|

Sources: [adeline/app/controller.py192-211](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L192-L211) [adeline/inference/roi/](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/inference/roi/)

### Initialization

Custom pipeline initialization passes the handler as `on_video_frame`:

```
self.pipeline = InferencePipeline.init_with_custom_logic(
    video_reference=self.config.RTSP_URL,
    on_video_frame=self.inference_handler,  # Custom wrapper
    on_prediction=on_prediction,
    max_fps=self.config.MAX_FPS,
    watchdog=self.watchdog,
    status_update_handlers=[self._status_update_handler],
)
```

The `on_video_frame` callable receives each frame before inference and must return predictions.

Sources: [adeline/app/controller.py238-245](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L238-L245)

---

## Frame Processing Flow

The following diagram shows the complete frame processing pipeline from video input to output sinks:

**Diagram: Frame Processing Sequence**

### Processing Steps

1. **Frame Acquisition**: Pipeline reads from RTSP stream at configured FPS
2. **Custom Preprocessing** (custom mode only):
    - Query ROI state for current crop region
    - Crop frame to ROI
3. **Inference**: Run YOLO model on frame (or cropped region)
4. **Coordinate Transformation** (custom mode only): Convert detections from ROI space to full frame space
5. **Stabilization** (if enabled): Apply temporal filtering and hysteresis
6. **Multi-Sink Distribution**: Fan out results to multiple destinations

Sources: [adeline/app/controller.py130-276](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L130-L276) [adeline/inference/roi/adaptive.py](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/inference/roi/adaptive.py) [adeline/inference/roi/fixed.py](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/inference/roi/fixed.py)

---

## Model Management

### Model Configuration

Models are configured in `config.yaml` under the `models` section:

```
models:
  use_local: true
  local_model_path: "models/yolov8n.onnx"
  model_id: "yolov8n-640"
  imgsz: 640
  confidence: 0.25
  iou_threshold: 0.45
```

|Parameter|Type|Description|
|---|---|---|
|`use_local`|boolean|If true, load from file system; if false, use Roboflow API|
|`local_model_path`|string|Path to ONNX model file (only if `use_local: true`)|
|`model_id`|string|Roboflow model identifier (only if `use_local: false`)|
|`imgsz`|integer|Model input size (width/height in pixels)|
|`confidence`|float|Minimum confidence threshold for detections|
|`iou_threshold`|float|IOU threshold for non-maximum suppression|

### Model Disabling

To prevent loading unused heavy models (which can cause memory issues), the system supports explicitly disabling models before imports:

```
models_disabled:
  disabled:
    - "yolov8"
    - "sam"
```

The `disable_models_from_config()` function must be called **before** importing the `inference` module:

```
from ..config import disable_models_from_config

# CRITICAL: Disable models BEFORE importing inference
disable_models_from_config()

# NOW safe to import
from inference import InferencePipeline
```

This pattern prevents `ModelDependencyMissing` warnings for models that are not needed.

Sources: [adeline/config.py](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py) [adeline/app/controller.py22-31](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L22-L31) [adeline/CLAUDE.md46-49](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/CLAUDE.md#L46-L49)

### Process Frame Functions

Different model types require different inference APIs. The `get_process_frame_function()` factory returns the appropriate callable:

**Diagram: Process Frame Function Selection**

The returned function is passed to inference handlers for uniform invocation regardless of model source.

Sources: [adeline/inference/models.py](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/inference/models.py) [adeline/app/controller.py189](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L189-L189)

---

## Inference Configuration

The `ModelConfig` object configures inference behavior:

```
from inference.core.interfaces.stream.entities import ModelConfig

inference_config = ModelConfig.init(
    confidence=self.config.MODEL_CONFIDENCE,
    iou_threshold=self.config.MODEL_IOU_THRESHOLD,
)
```

This configuration is passed to inference handlers and controls:

- **Confidence threshold**: Minimum score for detection to be included
- **IOU threshold**: Overlap threshold for non-maximum suppression

Sources: [adeline/app/controller.py183-186](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L183-L186)

---

## Sink Composition Pattern

The pipeline outputs results to multiple destinations simultaneously using the `multi_sink` pattern from the `inference` library:

**Diagram: Multi-Sink Pattern for Output Distribution**

### Sink Configuration

The sink list is assembled based on configuration:

```
sinks_list = [mqtt_sink]  # Always included

# ROI update sink only for adaptive mode
if self.config.ROI_MODE == 'adaptive':
    roi_sink = partial(roi_update_sink, roi_state=self.roi_state)
    sinks_list.append(roi_sink)

# Visualization sink if enabled
if self.config.ENABLE_VISUALIZATION:
    viz_sink = create_visualization_sink(...)
    sinks_list.append(viz_sink)

# Compose into multi-sink
on_prediction = partial(multi_sink, sinks=sinks_list)
```

The `on_prediction` callable is invoked by the pipeline for each frame's detections. The `multi_sink` wrapper fans out the call to all configured sinks in parallel.

### Sink Responsibilities

|Sink|Purpose|Always Active?|
|---|---|---|
|**MQTT Sink**|Publish detections to MQTT data plane|Yes|
|**Visualization Sink**|Display annotated frames in OpenCV window|If `visualization.enabled: true`|
|**ROI Update Sink**|Update adaptive ROI state from detections|If `roi_strategy.mode: adaptive`|

Sources: [adeline/app/controller.py213-234](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L213-L234) [adeline/data/sinks.py](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/data/sinks.py) [adeline/visualization/sinks.py](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/visualization/sinks.py)

---

## Stabilization Integration

When detection stabilization is enabled (`detection_stabilization.mode != "none"`), the MQTT sink is wrapped with a stabilization sink:

```
if self.config.STABILIZATION_MODE != 'none':
    from ..inference.stabilization import (
        create_stabilization_strategy,
        create_stabilization_sink,
        StabilizationConfig,
    )

    # Create stabilizer instance
    stab_config = StabilizationConfig(...)
    self.stabilizer = create_stabilization_strategy(stab_config)

    # Wrap mqtt_sink with stabilization
    mqtt_sink = create_stabilization_sink(
        stabilizer=self.stabilizer,
        downstream_sink=mqtt_sink,
    )
```

The stabilization sink:

1. Receives raw predictions from inference
2. Applies temporal filtering and hysteresis
3. Forwards only confirmed detections to downstream sink (MQTT)

This middleware pattern allows stabilization to be inserted transparently without modifying pipeline code. For stabilization details, see [Detection Stabilization](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/5.6-detection-stabilization).

Sources: [adeline/app/controller.py94-125](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L94-L125)

---

## Status Updates and Watchdog

The pipeline provides two monitoring mechanisms:

### Status Update Handler

The `_status_update_handler` callback receives status events from the pipeline:

```
def _status_update_handler(self, status: StatusUpdate):
    """Handler para status updates del pipeline"""
    if status.severity.value >= UpdateSeverity.WARNING.value:
        logger.warning(
            f"Pipeline Status: [{status.severity.name}] {status.event_type}"
        )
```

Status events include:

- Frame acquisition errors
- Inference failures
- Stream disconnections
- Performance warnings

The handler filters by severity and logs warnings/errors.

### Watchdog Metrics

The `BasePipelineWatchDog` instance collects performance metrics:

```
self.watchdog = BasePipelineWatchDog()  # Created in controller
```

Metrics tracked:

- Frames processed per second
- Inference latency
- Frame drops
- Total frames processed

The watchdog is connected to the Data Plane and publishes metrics when the `METRICS` command is received. See [Pipeline Controller](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/5.1-pipeline-controller) for metrics command handling.

Sources: [adeline/app/controller.py64](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L64-L64) [adeline/app/controller.py321-327](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L321-L327) [adeline/app/controller.py368-373](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L368-L373)

---

## Pipeline Lifecycle

The pipeline lifecycle is managed by the `InferencePipelineController`:

**Diagram: Pipeline Lifecycle State Machine**

### Lifecycle Methods

|Method|Purpose|Code Reference|
|---|---|---|
|`setup()`|Initialize pipeline and MQTT connections|[adeline/app/controller.py68-319](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L68-L319)|
|`run()`|Start pipeline and wait for shutdown|[adeline/app/controller.py436-474](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L436-L474)|
|`_handle_pause()`|Pause frame processing|[adeline/app/controller.py343-353](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L343-L353)|
|`_handle_resume()`|Resume frame processing|[adeline/app/controller.py355-365](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L355-L365)|
|`_handle_stop()`|Stop pipeline and trigger shutdown|[adeline/app/controller.py328-341](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L328-L341)|
|`cleanup()`|Release resources and disconnect|[adeline/app/controller.py489-527](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L489-L527)|

The pipeline automatically starts on initialization and runs continuously until stopped via MQTT command or signal interrupt.

Sources: [adeline/app/controller.py54-527](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L54-L527)

---

## Configuration Reference

### Pipeline Settings

```
pipeline:
  rtsp_url: "rtsp://localhost:8554/stream"
  max_fps: 30
  enable_visualization: true
  display_statistics: true
```

### Model Settings

```
models:
  use_local: true
  local_model_path: "models/yolov8n.onnx"
  model_id: "yolov8n-640"
  imgsz: 640
  confidence: 0.25
  iou_threshold: 0.45
```

### ROI Strategy Settings

```
roi_strategy:
  mode: "adaptive"  # or "fixed" or "none"
  
  # Adaptive mode parameters
  adaptive:
    margin: 0.2
    smoothing: 0.3
    # ... (see ROI Strategies page)
  
  # Fixed mode parameters
  fixed:
    x_min: 0.2
    y_min: 0.2
    x_max: 0.8
    y_max: 0.8
```

### Stabilization Settings

```
detection_stabilization:
  mode: "temporal"  # or "none"
  temporal:
    min_frames: 3
    max_gap: 2
    appear_conf: 0.5
    persist_conf: 0.3
```

For complete configuration reference, see [Configuration Reference](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/6-configuration-reference).

Sources: [adeline/config.py](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py) [adeline/CLAUDE.md44-75](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/CLAUDE.md#L44-L75)