# Running Your First Pipeline

Relevant source files

- [Makefile](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/Makefile)
- [adeline/__main__.py](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/__main__.py)
- [adeline/app/controller.py](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py)

## Purpose and Scope

This page guides you through starting the Adeline inference pipeline for the first time after completing installation ([2.1](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/2.1-installation-and-dependencies)) and configuration ([2.2](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/2.2-configuration-setup)). You will learn how to start the pipeline, interpret the initialization output, verify correct operation, and understand what's happening behind the scenes. For details on controlling a running pipeline, seeÂ [7.2](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/7.2-control-commands). For monitoring data streams, seeÂ [7.3](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/7.3-monitoring-and-data-streams).

---

## Starting the Pipeline

### Using Make

The simplest way to start the pipeline is via the Makefile command:

```
make run
```

This executesÂ `uv run python -m adeline`Â [Makefile74-77](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/Makefile#L74-L77)

**Alternative command:**

```
make start  # Alias for 'run'
```

### Direct Python Invocation

You can also start the pipeline directly:

```
python -m adeline
```

This invokes theÂ `adeline.__main__`Â moduleÂ [adeline/__main__.py1-7](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/__main__.py#L1-L7)Â which calls theÂ `main()`Â function fromÂ `adeline.app.controller`.

**Sources:**Â [Makefile74-79](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/Makefile#L74-L79)Â [adeline/__main__.py1-7](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/__main__.py#L1-L7)

---

## Initialization Sequence

When you run the pipeline, the system follows a strict initialization order to prevent dependency issues and ensure proper component setup.

### Initialization Flow Diagram

**Sources:**Â [adeline/app/controller.py533-558](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L533-L558)Â [adeline/app/controller.py59-319](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L59-L319)

---

## Console Output Walkthrough

### Phase 1: Configuration Loading

```
ğŸš€ Inicializando InferencePipeline con MQTT...
```

The controller initializes by loading configuration fromÂ `config.yaml`Â andÂ `.env`Â filesÂ [adeline/app/controller.py70](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L70-L70)

### Phase 2: Data Plane Connection

```
ğŸ“¡ Configurando Data Plane...
âœ… Data Plane conectado
```

TheÂ `MQTTDataPlane`Â connects to the MQTT broker atÂ `localhost:1883`Â (default) using QoS 0 for high-throughput data publishingÂ [adeline/app/controller.py74-89](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L74-L89)

|Component|Topic|QoS|Purpose|
|---|---|---|---|
|Data Plane|`inference/data/detections`|0|Detection results|
|Data Plane|`inference/data/metrics`|0|Pipeline metrics|

### Phase 3: Stabilization (Optional)

IfÂ `detection_stabilization.mode`Â is notÂ `none`:

```
ğŸ”§ Configurando Detection Stabilization...
âœ… Detection Stabilization habilitado: mode=temporal
```

The stabilization wrapper is applied to reduce detection flickeringÂ [adeline/app/controller.py95-125](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L95-L125)

### Phase 4: Pipeline Creation

Depending onÂ `roi_strategy.mode`:

**Standard Pipeline (mode:Â `none`):**

```
ğŸ“¦ Usando pipeline standard (default)
ğŸ”§ Creando InferencePipeline (standard)...
```

**Custom Pipeline with ROI (mode:Â `adaptive`Â orÂ `fixed`):**

```
ğŸ”„ Using AdaptiveInferenceHandler (dynamic ROI)
ğŸ”§ Creando InferencePipeline (custom logic)...
```

The pipeline is created with either standard Roboflow model inference or custom ROI/stabilization logicÂ [adeline/app/controller.py131-276](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L131-L276)

### Phase 5: Control Plane Connection

```
ğŸ® Configurando Control Plane...
âœ… Control Plane conectado
```

TheÂ `MQTTControlPlane`Â connects with QoS 1 for reliable command deliveryÂ [adeline/app/controller.py280-305](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L280-L305)

|Component|Topic|QoS|Purpose|
|---|---|---|---|
|Control Plane|`inference/control/commands`|1|Command reception|
|Control Plane|`inference/control/status`|1|Status publishing|

### Phase 6: Auto-Start

```
â–¶ï¸ Iniciando pipeline automÃ¡ticamente...
âœ… Pipeline iniciado y corriendo
âœ… Setup completado
```

The pipeline automatically starts inference without requiring a separate start commandÂ [adeline/app/controller.py308-318](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L308-L318)

### Phase 7: Running Status

```
======================================================================
ğŸ¬ InferencePipeline con MQTT activo y corriendo
======================================================================
ğŸ“¡ Control Topic: inference/control/commands
ğŸ“Š Data Topic: inference/data/detections
â–¶ï¸  Estado: RUNNING

ğŸ’¡ Comandos MQTT disponibles:
   PAUSE:   {"command": "pause"}   - Pausa el procesamiento
   RESUME:  {"command": "resume"}  - Reanuda el procesamiento
   STOP:    {"command": "stop"}    - Detiene y finaliza
   STATUS:  {"command": "status"}  - Consulta estado actual
   METRICS: {"command": "metrics"} - Publica mÃ©tricas del pipeline

âŒ¨ï¸  Presiona Ctrl+C para salir
======================================================================
```

The pipeline is now running and waiting for MQTT commands or keyboard interruptsÂ [adeline/app/controller.py442-459](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L442-L459)

**Sources:**Â [adeline/app/controller.py442-474](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L442-L474)

---

## Component Initialization Map

This diagram shows how code entities are initialized during startup:

**Sources:**Â [adeline/__main__.py1-7](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/__main__.py#L1-L7)Â [adeline/app/controller.py533-558](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L533-L558)Â [adeline/app/controller.py54-319](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L54-L319)

---

## Verifying Correct Operation

### Check 1: MQTT Broker Connection

You should see successful connection messages for both planes:

```
âœ… Data Plane conectado
âœ… Control Plane conectado
```

If you see connection errors, ensure the MQTT broker is running:

```
make services-status
```

If not running, start it:

```
make services-up
```

### Check 2: Pipeline Status

The console should display:

```
â–¶ï¸  Estado: RUNNING
```

This indicates the pipeline is actively processing video frames.

### Check 3: Video Stream

IfÂ `enable_visualization: true`Â in your config, an OpenCV window should appear showing:

- Live video feed from the RTSP stream
- Bounding boxes around detected objects
- Optional ROI overlay (if using adaptive/fixed modes)
- FPS and detection statistics (ifÂ `display_statistics: true`)

### Check 4: MQTT Data Publishing

In a separate terminal, monitor the data stream:

```
make monitor-data
```

You should see JSON messages with detection results flowing continuously.

**Sources:**Â [adeline/app/controller.py68-319](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L68-L319)Â [Makefile115-118](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/Makefile#L115-L118)

---

## What's Happening Behind the Scenes

### Video Frame Processing

**Sources:**Â [adeline/app/controller.py131-276](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L131-L276)

### Component Interaction During Runtime

**Sources:**Â [adeline/app/controller.py436-528](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L436-L528)Â [adeline/app/controller.py476-487](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L476-L487)

---

## Stopping the Pipeline

### Method 1: Keyboard Interrupt (Ctrl+C)

PressÂ `Ctrl+C`Â in the terminal running the pipeline. The signal handler triggers graceful shutdown:

```
âš ï¸ SeÃ±al de terminaciÃ³n recibida...
ğŸ›‘ Deteniendo pipeline...
âœ… Pipeline detenido
ğŸ§¹ Limpiando recursos...
âœ… Control Plane desconectado
âœ… Data Plane desconectado
ğŸ‘‹ Hasta luego!
```

The shutdown sequence:

1. Signal handler catchesÂ `SIGINT`Â [adeline/app/controller.py476-487](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L476-L487)
2. SetsÂ `shutdown_event`Â [adeline/app/controller.py479](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L479-L479)
3. CallsÂ `pipeline.terminate()`Â [adeline/app/controller.py484](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L484-L484)
4. RunsÂ `cleanup()`Â [adeline/app/controller.py489-527](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L489-L527)
5. Disconnects MQTT planesÂ [adeline/app/controller.py510-518](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L510-L518)
6. Forces exit withÂ `os._exit(0)`Â [adeline/app/controller.py527](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L527-L527)

### Method 2: MQTT Stop Command

From another terminal:

```
make stop
```

This sends aÂ `{"command": "stop"}`Â message via MQTTÂ [Makefile84-86](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/Makefile#L84-L86)Â triggering theÂ `_handle_stop()`Â callbackÂ [adeline/app/controller.py328-341](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L328-L341)

**Sources:**Â [adeline/app/controller.py476-527](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L476-L527)Â [Makefile84-86](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/Makefile#L84-L86)

---

## Common First-Run Issues

### Issue: MQTT Connection Failed

**Symptom:**

```
âŒ No se pudo conectar Data Plane
```

**Solution:**Â Start the MQTT broker:

```
make services-up
```

Verify it's running:

```
make services-status
```

### Issue: RTSP Stream Unavailable

**Symptom:**

```
Pipeline Status: [WARNING] STREAM_OPERATION_ERROR
```

**Solution:**

- Check thatÂ `rtsp_url`Â inÂ `config.yaml`Â points to a valid stream
- If using go2rtc proxy, ensure it's configured and running
- Test the RTSP URL with a tool like VLC or ffplay

### Issue: Model Loading Errors

**Symptom:**

```
ModelDependencyMissing warnings
```

**Solution:**Â This indicatesÂ `disable_models_from_config()`Â was not called before importing inference. This should not happen in normal operation. Verify your configuration is being loaded properly.

### Issue: Missing API Key

**Symptom:**

```
Error: API_KEY not found
```

**Solution:**Â Ensure yourÂ `.env`Â file exists and contains:

```
API_KEY=your_roboflow_api_key
```

**Sources:**Â [adeline/app/controller.py68-89](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L68-L89)

---

## Next Steps

Now that your pipeline is running:

1. **Send Control Commands**: Try pausing and resuming the pipeline. SeeÂ [7.2](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/7.2-control-commands)Â for all available commands.
    
2. **Monitor Data Streams**: Watch detection results in real-time. SeeÂ [7.3](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/7.3-monitoring-and-data-streams)Â for monitoring tools.
    
3. **Adjust Configuration**: Fine-tune ROI, stabilization, and inference parameters. SeeÂ [6.2](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/6.2-pipeline-settings)Â for configuration options.
    
4. **Understand the Architecture**: Learn how components interact. SeeÂ [3.3](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/3.3-component-overview)Â for detailed component documentation.
    

**Sources:**Â [Makefile84-124](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/Makefile#L84-L124)