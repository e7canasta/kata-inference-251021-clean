# Running Your First Pipeline

Relevant source files

- [Makefile](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/Makefile)
- [adeline/__main__.py](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/__main__.py)
- [adeline/app/controller.py](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py)

## Purpose and Scope

This page guides you through starting the Adeline inference pipeline for the first time after completing installation ([2.1](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/2.1-installation-and-dependencies)) and configuration ([2.2](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/2.2-configuration-setup)). You will learn how to start the pipeline, interpret the initialization output, verify correct operation, and understand what's happening behind the scenes. For details on controlling a running pipeline, see [7.2](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/7.2-control-commands). For monitoring data streams, see [7.3](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/7.3-monitoring-and-data-streams).

---

## Starting the Pipeline

### Using Make

The simplest way to start the pipeline is via the Makefile command:

```
make run
```

This executes `uv run python -m adeline` [Makefile74-77](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/Makefile#L74-L77)

**Alternative command:**

```
make start  # Alias for 'run'
```

### Direct Python Invocation

You can also start the pipeline directly:

```
python -m adeline
```

This invokes the `adeline.__main__` module [adeline/__main__.py1-7](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/__main__.py#L1-L7) which calls the `main()` function from `adeline.app.controller`.

**Sources:** [Makefile74-79](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/Makefile#L74-L79) [adeline/__main__.py1-7](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/__main__.py#L1-L7)

---

## Initialization Sequence

When you run the pipeline, the system follows a strict initialization order to prevent dependency issues and ensure proper component setup.

### Initialization Flow Diagram

**Sources:** [adeline/app/controller.py533-558](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L533-L558) [adeline/app/controller.py59-319](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L59-L319)

---

## Console Output Walkthrough

### Phase 1: Configuration Loading

```
🚀 Inicializando InferencePipeline con MQTT...
```

The controller initializes by loading configuration from `config.yaml` and `.env` files [adeline/app/controller.py70](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L70-L70)

### Phase 2: Data Plane Connection

```
📡 Configurando Data Plane...
✅ Data Plane conectado
```

The `MQTTDataPlane` connects to the MQTT broker at `localhost:1883` (default) using QoS 0 for high-throughput data publishing [adeline/app/controller.py74-89](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L74-L89)

|Component|Topic|QoS|Purpose|
|---|---|---|---|
|Data Plane|`inference/data/detections`|0|Detection results|
|Data Plane|`inference/data/metrics`|0|Pipeline metrics|

### Phase 3: Stabilization (Optional)

If `detection_stabilization.mode` is not `none`:

```
🔧 Configurando Detection Stabilization...
✅ Detection Stabilization habilitado: mode=temporal
```

The stabilization wrapper is applied to reduce detection flickering [adeline/app/controller.py95-125](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L95-L125)

### Phase 4: Pipeline Creation

Depending on `roi_strategy.mode`:

**Standard Pipeline (mode: `none`):**

```
📦 Usando pipeline standard (default)
🔧 Creando InferencePipeline (standard)...
```

**Custom Pipeline with ROI (mode: `adaptive` or `fixed`):**

```
🔄 Using AdaptiveInferenceHandler (dynamic ROI)
🔧 Creando InferencePipeline (custom logic)...
```

The pipeline is created with either standard Roboflow model inference or custom ROI/stabilization logic [adeline/app/controller.py131-276](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L131-L276)

### Phase 5: Control Plane Connection

```
🎮 Configurando Control Plane...
✅ Control Plane conectado
```

The `MQTTControlPlane` connects with QoS 1 for reliable command delivery [adeline/app/controller.py280-305](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L280-L305)

|Component|Topic|QoS|Purpose|
|---|---|---|---|
|Control Plane|`inference/control/commands`|1|Command reception|
|Control Plane|`inference/control/status`|1|Status publishing|

### Phase 6: Auto-Start

```
▶️ Iniciando pipeline automáticamente...
✅ Pipeline iniciado y corriendo
✅ Setup completado
```

The pipeline automatically starts inference without requiring a separate start command [adeline/app/controller.py308-318](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L308-L318)

### Phase 7: Running Status

```
======================================================================
🎬 InferencePipeline con MQTT activo y corriendo
======================================================================
📡 Control Topic: inference/control/commands
📊 Data Topic: inference/data/detections
▶️  Estado: RUNNING

💡 Comandos MQTT disponibles:
   PAUSE:   {"command": "pause"}   - Pausa el procesamiento
   RESUME:  {"command": "resume"}  - Reanuda el procesamiento
   STOP:    {"command": "stop"}    - Detiene y finaliza
   STATUS:  {"command": "status"}  - Consulta estado actual
   METRICS: {"command": "metrics"} - Publica métricas del pipeline

⌨️  Presiona Ctrl+C para salir
======================================================================
```

The pipeline is now running and waiting for MQTT commands or keyboard interrupts [adeline/app/controller.py442-459](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L442-L459)

**Sources:** [adeline/app/controller.py442-474](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L442-L474)

---

## Component Initialization Map

This diagram shows how code entities are initialized during startup:

**Sources:** [adeline/__main__.py1-7](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/__main__.py#L1-L7) [adeline/app/controller.py533-558](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L533-L558) [adeline/app/controller.py54-319](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L54-L319)

---

## Verifying Correct Operation

### Check 1: MQTT Broker Connection

You should see successful connection messages for both planes:

```
✅ Data Plane conectado
✅ Control Plane conectado
```

If you see connection errors, ensure the MQTT broker is running:

```
make services-status
```

If not running, start it:

```
make services-up
```

### Check 2: Pipeline Status

The console should display:

```
▶️  Estado: RUNNING
```

This indicates the pipeline is actively processing video frames.

### Check 3: Video Stream

If `enable_visualization: true` in your config, an OpenCV window should appear showing:

- Live video feed from the RTSP stream
- Bounding boxes around detected objects
- Optional ROI overlay (if using adaptive/fixed modes)
- FPS and detection statistics (if `display_statistics: true`)

### Check 4: MQTT Data Publishing

In a separate terminal, monitor the data stream:

```
make monitor-data
```

You should see JSON messages with detection results flowing continuously.

**Sources:** [adeline/app/controller.py68-319](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L68-L319) [Makefile115-118](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/Makefile#L115-L118)

---

## What's Happening Behind the Scenes

### Video Frame Processing

**Sources:** [adeline/app/controller.py131-276](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L131-L276)

### Component Interaction During Runtime

**Sources:** [adeline/app/controller.py436-528](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L436-L528) [adeline/app/controller.py476-487](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L476-L487)

---

## Stopping the Pipeline

### Method 1: Keyboard Interrupt (Ctrl+C)

Press `Ctrl+C` in the terminal running the pipeline. The signal handler triggers graceful shutdown:

```
⚠️ Señal de terminación recibida...
🛑 Deteniendo pipeline...
✅ Pipeline detenido
🧹 Limpiando recursos...
✅ Control Plane desconectado
✅ Data Plane desconectado
👋 Hasta luego!
```

The shutdown sequence:

1. Signal handler catches `SIGINT` [adeline/app/controller.py476-487](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L476-L487)
2. Sets `shutdown_event` [adeline/app/controller.py479](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L479-L479)
3. Calls `pipeline.terminate()` [adeline/app/controller.py484](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L484-L484)
4. Runs `cleanup()` [adeline/app/controller.py489-527](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L489-L527)
5. Disconnects MQTT planes [adeline/app/controller.py510-518](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L510-L518)
6. Forces exit with `os._exit(0)` [adeline/app/controller.py527](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L527-L527)

### Method 2: MQTT Stop Command

From another terminal:

```
make stop
```

This sends a `{"command": "stop"}` message via MQTT [Makefile84-86](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/Makefile#L84-L86) triggering the `_handle_stop()` callback [adeline/app/controller.py328-341](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L328-L341)

**Sources:** [adeline/app/controller.py476-527](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L476-L527) [Makefile84-86](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/Makefile#L84-L86)

---

## Common First-Run Issues

### Issue: MQTT Connection Failed

**Symptom:**

```
❌ No se pudo conectar Data Plane
```

**Solution:** Start the MQTT broker:

```
make services-up
```

Verify it's running:

```
make services-status
```

### Issue: RTSP Stream Unavailable

**Symptom:**

```
Pipeline Status: [WARNING] STREAM_OPERATION_ERROR
```

**Solution:**

- Check that `rtsp_url` in `config.yaml` points to a valid stream
- If using go2rtc proxy, ensure it's configured and running
- Test the RTSP URL with a tool like VLC or ffplay

### Issue: Model Loading Errors

**Symptom:**

```
ModelDependencyMissing warnings
```

**Solution:** This indicates `disable_models_from_config()` was not called before importing inference. This should not happen in normal operation. Verify your configuration is being loaded properly.

### Issue: Missing API Key

**Symptom:**

```
Error: API_KEY not found
```

**Solution:** Ensure your `.env` file exists and contains:

```
API_KEY=your_roboflow_api_key
```

**Sources:** [adeline/app/controller.py68-89](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L68-L89)

---

## Next Steps

Now that your pipeline is running:

1. **Send Control Commands**: Try pausing and resuming the pipeline. See [7.2](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/7.2-control-commands) for all available commands.
    
2. **Monitor Data Streams**: Watch detection results in real-time. See [7.3](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/7.3-monitoring-and-data-streams) for monitoring tools.
    
3. **Adjust Configuration**: Fine-tune ROI, stabilization, and inference parameters. See [6.2](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/6.2-pipeline-settings) for configuration options.
    
4. **Understand the Architecture**: Learn how components interact. See [3.3](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/3.3-component-overview) for detailed component documentation.
    

**Sources:** [Makefile84-124](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/Makefile#L84-L124)