# Configuration Setup

Relevant source files

- [README.md](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/README.md)
- [adeline/CLAUDE.md](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/CLAUDE.md)
- [adeline/config.py](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py)

This guide walks you through creating and configuring the required configuration files for the Adeline inference system. You will learn how to set up environment variables and pipeline settings to prepare the system for first run.

For comprehensive documentation of all configuration options, see [Configuration Reference](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/6-configuration-reference). For information about the conceptual architecture of the configuration system, see [Configuration-Driven Architecture](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/4.1-configuration-driven-architecture) and [Configuration Hierarchy](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/6.1-configuration-hierarchy).

---

## Purpose and Scope

This page covers the practical steps needed to configure the Adeline system for initial operation:

- Creating `.env` and `config.yaml` files from examples
- Understanding the three-tier configuration hierarchy
- Setting required vs optional configuration values
- Understanding the configuration loading process
- Verifying configuration before first run

---

## Configuration Hierarchy

The Adeline system uses a three-tier configuration architecture that separates concerns by type and sensitivity level.

### Configuration Tiers

**Sources:** [adeline/config.py1-206](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L1-L206) [README.md174-180](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/README.md#L174-L180) [adeline/CLAUDE.md44-54](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/CLAUDE.md#L44-L54)

The three configuration tiers serve distinct purposes:

|Tier|Files|Purpose|Version Control|
|---|---|---|---|
|**Secrets**|`.env`|API keys, passwords, sensitive credentials|`.gitignore` (excluded)|
|**Settings**|`config.yaml`|Pipeline behavior, MQTT topics, strategies|`.gitignore` (excluded)|
|**Infrastructure**|`docker-compose.yml`, `mosquitto.conf`, `go2rtc.yaml`|Service definitions, network configuration|Committed to repository|

---

## Setting Up Environment Variables

### Creating the .env File

The `.env` file stores sensitive credentials that should never be committed to version control.

**Step 1: Copy the example file**

```
cp .env.example .env
```

**Step 2: Edit required variables**

Open `.env` in a text editor and set the following required values:

```
# Roboflow API (required if using cloud models)
ROBOFLOW_API_KEY=your_api_key_here

# MQTT Broker Authentication (optional, depends on broker config)
MQTT_USERNAME=your_username
MQTT_PASSWORD=your_password
```

### Environment Variable Reference

|Variable|Required|Used By|Purpose|
|---|---|---|---|
|`ROBOFLOW_API_KEY`|Conditional*|Model loader|Authenticate with Roboflow API for cloud model inference|
|`MQTT_USERNAME`|Optional|MQTT clients|Authenticate with MQTT broker (if broker requires auth)|
|`MQTT_PASSWORD`|Optional|MQTT clients|Authenticate with MQTT broker (if broker requires auth)|

* Required only if `models.use_local: false` in `config.yaml`. Can be omitted when using local ONNX models.

### How Environment Variables Are Loaded

**Sources:** [adeline/config.py14](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L14-L14) [adeline/config.py94-110](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L94-L110)

The `load_dotenv()` function is called at module import time [adeline/config.py14](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L14-L14) ensuring environment variables are available before `PipelineConfig` initialization. The `PipelineConfig.__init__()` method retrieves these values using `os.getenv()` [adeline/config.py94-110](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L94-L110)

---

## Setting Up Pipeline Configuration

### Creating the config.yaml File

The `config.yaml` file defines pipeline behavior, model selection, MQTT topics, and processing strategies.

**Step 1: Copy the example file**

```
cp config/adeline/config.yaml.example config/adeline/config.yaml
```

**Step 2: Verify location**

The file must be located at `config/adeline/config.yaml`. The `PipelineConfig` class expects this path by default [adeline/config.py59](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L59-L59)

### Configuration File Structure

The `config.yaml` file is organized into logical sections that correspond to system components:

**Sources:** [adeline/config.py56-206](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L56-L206)

### Essential Configuration Sections

#### 1. Pipeline Settings

Defines video input and basic inference parameters:

```
pipeline:
  rtsp_url: "rtsp://127.0.0.1:8554/live"  # RTSP stream URL
  model_id: "yolov11n-640"                # Model identifier
  max_fps: 2                              # Maximum inference FPS
  enable_visualization: true              # Show OpenCV window
  display_statistics: true                # Show FPS/latency stats
```

These values are loaded into `PipelineConfig` attributes [adeline/config.py78-83](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L78-L83):

- `self.RTSP_URL`
- `self.MODEL_ID`
- `self.MAX_FPS`
- `self.ENABLE_VISUALIZATION`
- `self.DISPLAY_STATISTICS`

#### 2. Model Configuration

Controls model loading and inference parameters:

```
models:
  use_local: false                        # Use local ONNX (true) or Roboflow API (false)
  local_path: "models/yolov11n-320.onnx" # Path to local ONNX file
  imgsz: 320                              # Input image size
  confidence: 0.25                        # Detection confidence threshold
  iou_threshold: 0.45                     # IOU threshold for NMS
```

When `use_local: true`, the system loads models from the filesystem and does not require `ROBOFLOW_API_KEY`. When `false`, models are downloaded via Roboflow API [adeline/config.py94-100](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L94-L100)

#### 3. MQTT Configuration

Defines broker connection and topic structure:

```
mqtt:
  broker:
    host: "localhost"
    port: 1883
    # username/password loaded from .env if present
  
  topics:
    control_commands: "inference/control/commands"
    control_status: "inference/control/status"
    data: "inference/data/detections"
    metrics: "inference/data/metrics"
  
  qos:
    control: 1  # QoS 1 for reliable command delivery
    data: 0     # QoS 0 for high-throughput data
```

These settings configure the control plane (QoS 1) and data plane (QoS 0) as described in [MQTT Communication Architecture](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/3.2-mqtt-communication-architecture).

#### 4. ROI Strategy

Configures Region of Interest processing (see [ROI Strategies](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/5.5-roi-strategies)):

```
roi_strategy:
  mode: "adaptive"  # Options: "none", "adaptive", "fixed"
  
  adaptive:
    margin: 0.2
    smoothing: 0.3
    min_roi_multiple: 1
    max_roi_multiple: 4
  
  fixed:
    x_min: 0.2
    y_min: 0.2
    x_max: 0.8
    y_max: 0.8
```

The `mode` value determines which strategy factory creates [adeline/config.py157](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L157-L157)

#### 5. Detection Stabilization

Reduces detection flickering (see [Detection Stabilization](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/5.6-detection-stabilization)):

```
detection_stabilization:
  mode: "temporal"  # Options: "none", "temporal"
  
  temporal:
    min_frames: 3
    max_gap: 2
  
  hysteresis:
    appear_confidence: 0.5
    persist_confidence: 0.3
```

Loaded by `PipelineConfig` [adeline/config.py136-147](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L136-L147) and used by stabilization strategy factory.

#### 6. Model Disabling

Critical section for preventing heavy model loading:

```
models_disabled:
  disabled:
    - PALIGEMMA
    - FLORENCE2
    - QWEN_2_5
    - CORE_MODEL_SAM
    - CORE_MODEL_SAM2
    - CORE_MODEL_CLIP
    # ... additional models
```

This section is read by `disable_models_from_config()` [adeline/config.py46-50](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L46-L50) **before** the inference module is imported. Each model name is converted to an environment variable `{MODEL}_ENABLED=False`.

---

## Configuration Loading Process

### Critical Initialization Order

The configuration system enforces a specific initialization sequence to prevent `ModelDependencyMissing` warnings from heavy models.

**Sources:** [adeline/config.py22-51](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L22-L51) [adeline/CLAUDE.md44-49](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/CLAUDE.md#L44-L49)

### Why This Order Matters

The inference library attempts to load heavy models (SAM, CLIP, Florence2, etc.) at import time. If these models are not available, warnings are generated. By setting `{MODEL}_ENABLED=False` environment variables **before** importing the inference module, we prevent these warnings.

The pattern is documented in [adeline/config.py20-28](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L20-L28):

```
# ============================================================================
# MODEL DISABLING (before importing inference)
# ============================================================================
def disable_models_from_config(config_path: str = "config/adeline/config.yaml"):
    """
    Lee config.yaml y deshabilita modelos antes de importar inference.
    Esto previene los warnings de ModelDependencyMissing.

    Esta función debe ser llamada ANTES de importar el módulo inference.
    """
```

### PipelineConfig Class Instantiation

After model disabling, the `PipelineConfig` class is instantiated to load all configuration settings:

**Sources:** [adeline/config.py56-206](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L56-L206)

---

## Verifying Configuration

### Pre-flight Checks

Before running the pipeline, verify your configuration is correct:

**1. Check file existence:**

```
# Both files must exist
ls -l .env
ls -l config/adeline/config.yaml
```

**2. Verify RTSP source (if using video stream):**

```
# Test RTSP URL is accessible
ffplay rtsp://127.0.0.1:8554/live
# Press 'q' to quit
```

**3. Test MQTT broker connection:**

```
# Start MQTT broker if not running
make services-up

# Verify broker is listening
nc -zv localhost 1883
```

**4. Test Python imports:**

```
# This will fail if config is invalid
python -c "from adeline.config import disable_models_from_config, PipelineConfig; \
           disable_models_from_config(); \
           config = PipelineConfig(); \
           print('✓ Configuration loaded successfully')"
```

### Common Configuration Errors

|Error|Cause|Solution|
|---|---|---|
|`FileNotFoundError: Config file not found`|`config.yaml` missing|Copy from `config.yaml.example`|
|`ROBOFLOW_API_KEY not found in environment`|`.env` not set and using cloud models|Set `ROBOFLOW_API_KEY` in `.env` or set `models.use_local: true`|
|`MQTT connection refused`|Broker not running|Run `make services-up` to start broker|
|`ModelDependencyMissing warnings`|`disable_models_from_config()` not called before imports|Ensure initialization order is correct (see [Initialization Sequence](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/3.4-initialization-sequence))|

---

## Configuration Flow Summary

The complete configuration flow from files to running system:

**Sources:** [adeline/config.py1-206](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L1-L206) [adeline/CLAUDE.md44-54](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/CLAUDE.md#L44-L54) [README.md66-76](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/README.md#L66-L76)

---

## Next Steps

With configuration complete, you are ready to:

1. **Start infrastructure services**: See [Infrastructure Services](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/7.4-infrastructure-services)
2. **Run your first pipeline**: See [Running Your First Pipeline](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/2.3-running-your-first-pipeline)
3. **Explore configuration options**: See [Configuration Reference](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/6-configuration-reference)

The configuration system enables flexible deployment without code changes. You can adjust RTSP sources, model selection, ROI strategies, and MQTT topics entirely through `config.yaml`, making the system adaptable to different deployment environments.