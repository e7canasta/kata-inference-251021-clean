# Configuration Hierarchy

Relevant source files

- [README.md](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/README.md)
- [adeline/CLAUDE.md](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/CLAUDE.md)
- [adeline/config.py](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py)

## Purpose and Scope

This document explains the three-tier configuration system used by the Adeline inference pipeline. The hierarchy separates sensitive credentials, application settings, and infrastructure definitions into distinct files, enabling secure and flexible deployment. This page covers the structure and loading order of configuration sources, but not the specific pipeline settings themselves. For details on individual pipeline configuration options, see [Pipeline Settings](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/6.2-pipeline-settings). For MQTT-specific configuration, see [MQTT Configuration](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/6.4-mqtt-configuration).

## Configuration Tiers Overview

The Adeline system uses a three-tier configuration hierarchy that separates concerns and follows security best practices:

**Configuration Tier Summary**

|Tier|File(s)|Purpose|Version Control|Example|
|---|---|---|---|---|
|1|`.env`|Sensitive credentials (API keys, passwords)|`.gitignore`'d|`.env.example`|
|2|`config.yaml`|Application behavior, pipeline settings|`.gitignore`'d|`config.yaml.example`|
|3|`docker-compose.yml`, service configs|Infrastructure services|Committed|N/A|

Sources: [adeline/config.py1-206](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L1-L206) [adeline/CLAUDE.md44-54](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/CLAUDE.md#L44-L54) [README.md172-179](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/README.md#L172-L179)

## Tier 1: Environment Variables (.env)

The `.env` file contains sensitive data that should never be committed to version control. This file is loaded automatically via `python-dotenv` at module initialization.

### Environment Variables Structure

```
# .env file format
ROBOFLOW_API_KEY=your_api_key_here
MQTT_USERNAME=mqtt_user
MQTT_PASSWORD=mqtt_password
```

### Usage in Configuration Loading

The `PipelineConfig` class loads environment variables through the `os.getenv()` function after `load_dotenv()` executes:

```
# From config.py
load_dotenv()  # Line 14

# Later in PipelineConfig.__init__:
self.API_KEY = os.getenv('ROBOFLOW_API_KEY')  # Line 94
self.MQTT_USERNAME = os.getenv('MQTT_USERNAME') or broker_cfg.get('username')  # Line 109
self.MQTT_PASSWORD = os.getenv('MQTT_PASSWORD') or broker_cfg.get('password')  # Line 110
```

### Validation Rules

- **ROBOFLOW_API_KEY**: Required if `models.use_local: false` (using Roboflow cloud models). If missing, raises `ValueError` at [adeline/config.py96-100](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L96-L100)
- **MQTT_USERNAME/PASSWORD**: Optional. Falls back to values in `config.yaml` if not set in environment

### Template File

The `.env.example` file serves as a template showing all expected variables:

```
# Copy and customize:
cp .env.example .env
```

Sources: [adeline/config.py13-14](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L13-L14) [adeline/config.py94-110](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L94-L110) [README.md48-49](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/README.md#L48-L49)

## Tier 2: YAML Configuration (config.yaml)

The `config.yaml` file defines all non-sensitive application behavior. This is the primary configuration file that users edit to customize pipeline operation.

### Configuration File Location

The default path is `config/adeline/config.yaml`. This is loaded by the `PipelineConfig` class:

```
# Default path in PipelineConfig.__init__
def __init__(self, config_path: str = "config/adeline/config.yaml"):
```

If the file is missing, `PipelineConfig` raises a `FileNotFoundError` with guidance at [adeline/config.py69-72](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L69-L72)

### YAML Structure Categories

The configuration is organized into logical sections:

```
# High-level structure
pipeline:           # Core pipeline settings (RTSP, model ID, FPS, visualization)
models:             # Model loading (local vs Roboflow, confidence, IOU)
models_disabled:    # Explicit model disabling (prevents heavy model loading)
mqtt:               # MQTT broker, topics, QoS settings
detection_stabilization:  # Temporal filtering and hysteresis
roi_strategy:       # Region of interest mode (none, adaptive, fixed)
logging:            # Log levels and formats
```

### Configuration Loading Process

Sources: [adeline/config.py59-76](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L59-L76) [adeline/config.py22-51](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L22-L51)

## Tier 3: Infrastructure Configuration

Infrastructure services (MQTT broker, RTSP proxy) are configured independently of the Python application.

### Docker Compose Services

The `docker/adeline/docker-compose.mqtt.yml` file defines the MQTT broker service:

```
# Example structure (not actual file content)
services:
  mosquitto:
    image: eclipse-mosquitto
    ports:
      - "1883:1883"
    volumes:
      - ./mosquitto.conf:/mosquitto/config/mosquitto.conf
```

### Service Management

```
# Start infrastructure services
make services-up

# Check status
make services-status

# View logs
make services-logs

# Stop services
make services-down
```

### MQTT Broker Configuration

The `docker/adeline/mosquitto.conf` file configures the Mosquitto broker behavior (authentication, listeners, logging).

### RTSP Proxy Configuration

The `config/adeline/go2rtc.yaml` configures the go2rtc RTSP proxy for video streaming.

Sources: [README.md181-199](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/README.md#L181-L199) [README.md8-18](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/README.md#L8-L18)

## Critical Initialization Order

The configuration system enforces a critical initialization sequence to prevent `ModelDependencyMissing` warnings from heavy models.

### The Model Disabling Pattern

### disable_models_from_config() Function

This function at [adeline/config.py22-51](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L22-L51) is the key to the pattern:

**Purpose**: Read `models_disabled.disabled[]` from `config.yaml` and set environment variables BEFORE importing the `inference` module.

**Default Behavior**: If `config.yaml` doesn't exist, disables a default list of heavy models:

```
default_disabled = [
    "PALIGEMMA", "FLORENCE2", "QWEN_2_5",
    "CORE_MODEL_SAM", "CORE_MODEL_SAM2", "CORE_MODEL_CLIP",
    "CORE_MODEL_GAZE", "SMOLVLM2", "DEPTH_ESTIMATION",
    "MOONDREAM2", "CORE_MODEL_TROCR", "CORE_MODEL_GROUNDINGDINO",
    "CORE_MODEL_YOLO_WORLD", "CORE_MODEL_PE",
]
```

**Implementation**:

```
for model in disabled_models:
    os.environ[f"{model}_ENABLED"] = "False"
```

### Why This Order Matters

The `inference` library checks these environment variables during module import. If they're not set before import, the library attempts to load all models, causing:

1. Heavy memory usage
2. `ModelDependencyMissing` warnings for unavailable dependencies
3. Slower startup times

**Correct Usage Pattern**:

```
# In application entry point
from adeline.config import disable_models_from_config

# STEP 1: Disable models FIRST
disable_models_from_config()

# STEP 2: Now safe to import inference
from inference import InferencePipeline
```

Sources: [adeline/config.py22-51](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L22-L51) [adeline/CLAUDE.md44-49](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/CLAUDE.md#L44-L49) [adeline/CLAUDE.md106-109](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/CLAUDE.md#L106-L109)

## Configuration Validation

The `PipelineConfig` class performs validation during initialization:

### Required Value Checks

|Configuration|Validation Rule|Error Type|
|---|---|---|
|`config.yaml` file|Must exist|`FileNotFoundError`|
|`ROBOFLOW_API_KEY`|Required if `use_local: false`|`ValueError`|
|All other fields|Use defaults if missing|None (silent)|

### Validation Code Path

```
# File existence check (Line 68-72)
if not config_file.exists():
    raise FileNotFoundError(
        f"Config file not found: {config_path}\n"
        f"Please create it from config/adeline/config.yaml.example"
    )

# API key validation (Line 95-100)
if not self.USE_LOCAL_MODEL and not self.API_KEY:
    raise ValueError(
        "ROBOFLOW_API_KEY not found in environment variables.\n"
        "Please set it in your .env file (copy from .env.example)\n"
        "Or set models.use_local: true to use local ONNX models"
    )
```

### Default Values

Most configuration options have sensible defaults defined in the `PipelineConfig.__init__()` method using the `.get(key, default)` pattern:

```
self.MAX_FPS = pipeline_cfg.get('max_fps', 2)
self.ENABLE_VISUALIZATION = pipeline_cfg.get('enable_visualization', True)
self.MODEL_CONFIDENCE = models_cfg.get('confidence', 0.25)
```

Sources: [adeline/config.py68-100](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L68-L100)

## Backward Compatibility

The configuration system includes backward compatibility for legacy configuration structures:

### Legacy adaptive_crop.enabled → roi_strategy.mode

The system detects the old `adaptive_crop.enabled` structure and converts it to the new `roi_strategy.mode` structure:

```
# New structure (preferred)
roi_strategy:
  mode: adaptive  # or "fixed" or "none"
  adaptive:
    margin: 0.2
    smoothing: 0.3

# Legacy structure (still supported)
adaptive_crop:
  enabled: true
  margin: 0.2
  smoothing: 0.3
```

### Compatibility Logic

At [adeline/config.py152-205](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L152-L205) the config loader checks for the new structure first:

```
roi_strategy_cfg = config.get('roi_strategy', {})

if roi_strategy_cfg:
    # New structure: use roi_strategy.mode
    self.ROI_MODE = roi_strategy_cfg.get('mode', 'none').lower()
else:
    # Backward compatibility: Legacy adaptive_crop.enabled
    adaptive_crop_cfg = config.get('adaptive_crop', {})
    legacy_enabled = adaptive_crop_cfg.get('enabled', False)
    self.ROI_MODE = 'adaptive' if legacy_enabled else 'none'
    
    if legacy_enabled:
        logger.warning("⚠️ Using legacy 'adaptive_crop.enabled' config...")
```

This ensures existing configurations continue to work while encouraging migration to the new structure.

Sources: [adeline/config.py152-205](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L152-L205)

## Configuration Access Pattern

Once loaded, the configuration object is passed to the `InferencePipelineController`, which distributes settings to child components:

**Component Access Examples**:

- `MQTTControlPlane` accesses: `config.MQTT_BROKER`, `config.CONTROL_COMMAND_TOPIC`, `config.CONTROL_QOS`
- `MQTTDataPlane` accesses: `config.DATA_TOPIC`, `config.METRICS_TOPIC`, `config.DATA_QOS`
- `InferencePipeline` accesses: `config.MODEL_ID`, `config.RTSP_URL`, `config.MAX_FPS`
- ROI factories access: `config.ROI_MODE`, `config.CROP_MARGIN`, `config.FIXED_X_MIN`
- Stabilization factories access: `config.STABILIZATION_MODE`, `config.STABILIZATION_MIN_FRAMES`

Sources: [adeline/CLAUDE.md39-43](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/CLAUDE.md#L39-L43) [adeline/config.py56-206](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L56-L206)

## Configuration Files Summary

|File|Path|Purpose|Required|
|---|---|---|---|
|`.env`|`./env`|Sensitive credentials|Yes (if using Roboflow API)|
|`.env.example`|`./.env.example`|Template for `.env`|No (documentation)|
|`config.yaml`|`config/adeline/config.yaml`|Pipeline settings|Yes|
|`config.yaml.example`|`config/adeline/config.yaml.example`|Template for `config.yaml`|No (documentation)|
|`docker-compose.mqtt.yml`|`docker/adeline/docker-compose.mqtt.yml`|MQTT broker service|Yes (if using MQTT)|
|`mosquitto.conf`|`docker/adeline/mosquitto.conf`|MQTT broker config|Yes (if using MQTT)|
|`go2rtc.yaml`|`config/adeline/go2rtc.yaml`|RTSP proxy config|Optional|

Sources: [README.md7-52](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/README.md#L7-L52) [adeline/CLAUDE.md132-136](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/CLAUDE.md#L132-L136)