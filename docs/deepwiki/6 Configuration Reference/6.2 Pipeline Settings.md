# Pipeline Settings

Relevant source files

- [adeline/CLAUDE.md](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/CLAUDE.md)
- [adeline/config.py](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py)

This document details all configuration options available in `config.yaml` for controlling the inference pipeline behavior, including video input settings, model inference parameters, ROI strategies, detection stabilization, and logging. These settings control the core processing characteristics of the Adeline inference system.

For information about the overall configuration hierarchy and file structure, see [Configuration Hierarchy](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/6.1-configuration-hierarchy). For model loading and the `models_disabled` section, see [Model Management](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/6.3-model-management). For MQTT broker and topic configuration, see [MQTT Configuration](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/6.4-mqtt-configuration).

---

## Configuration Loading Architecture

The `PipelineConfig` class in [adeline/config.py56-206](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L56-L206) loads all pipeline settings from `config.yaml` and validates them during system initialization. The configuration is parsed before the inference pipeline starts, ensuring all parameters are available during component creation.

### Configuration to Code Mapping

**Sources:** [adeline/config.py56-206](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L56-L206)

---

## Pipeline Basic Settings

The `pipeline` section in `config.yaml` controls fundamental video processing parameters, including the video source, model selection, frame rate, and visualization options.

### Settings Reference

|Configuration Key|Python Attribute|Type|Default|Description|
|---|---|---|---|---|
|`pipeline.rtsp_url`|`RTSP_URL`|string|`rtsp://127.0.0.1:8554/live`|Video source URL (RTSP stream)|
|`pipeline.model_id`|`MODEL_ID`|string|`yolov11n-640`|Roboflow model identifier|
|`pipeline.max_fps`|`MAX_FPS`|int|`2`|Maximum frames per second to process|
|`pipeline.enable_visualization`|`ENABLE_VISUALIZATION`|boolean|`true`|Enable OpenCV window display|
|`pipeline.display_statistics`|`DISPLAY_STATISTICS`|boolean|`true`|Show FPS and metrics on display|

### Implementation Details

**RTSP URL**: The video source is configured via `pipeline.rtsp_url` [adeline/config.py79](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L79-L79) This typically points to a go2rtc proxy server that handles RTSP stream management. The URL is passed directly to the `InferencePipeline` initialization.

**Model ID**: When using Roboflow cloud inference (`models.use_local: false`), the `pipeline.model_id` [adeline/config.py80](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L80-L80) specifies which pre-trained model to load. This must match a valid Roboflow model identifier.

**FPS Limiting**: The `pipeline.max_fps` parameter [adeline/config.py81](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L81-L81) controls the inference rate to prevent overwhelming the system. A value of `2` means the pipeline processes approximately 2 frames per second, regardless of the video stream's native frame rate.

**Visualization**: The `pipeline.enable_visualization` flag [adeline/config.py82](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L82-L82) determines whether an OpenCV window is created to display annotated frames. When `false`, the pipeline runs in headless mode, suitable for server deployments.

**Statistics Display**: The `pipeline.display_statistics` flag [adeline/config.py83](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L83-L83) controls whether FPS and detection count overlays appear on the visualization window.

### Example Configuration

```
pipeline:
  rtsp_url: "rtsp://127.0.0.1:8554/camera1"
  model_id: "yolov11n-640"
  max_fps: 2
  enable_visualization: true
  display_statistics: true
```

**Sources:** [adeline/config.py78-84](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L78-L84)

---

## Model Inference Settings

The `models` section configures inference behavior, including whether to use local ONNX models or Roboflow cloud API, model resolution, and detection thresholds.

### Settings Reference

|Configuration Key|Python Attribute|Type|Default|Description|
|---|---|---|---|---|
|`models.use_local`|`USE_LOCAL_MODEL`|boolean|`false`|Use local ONNX model instead of Roboflow API|
|`models.local_path`|`LOCAL_MODEL_PATH`|string|`models/yolov11n-320.onnx`|Path to local ONNX model file|
|`models.imgsz`|`MODEL_IMGSZ`|int|`320`|Model input size (width/height in pixels)|
|`models.confidence`|`MODEL_CONFIDENCE`|float|`0.25`|Minimum confidence threshold for detections|
|`models.iou_threshold`|`MODEL_IOU_THRESHOLD`|float|`0.45`|IOU threshold for non-maximum suppression|

### Local vs Cloud Inference

**Implementation**: The `USE_LOCAL_MODEL` flag [adeline/config.py87](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L87-L87) determines the model source. When `true`, the system loads an ONNX model from `LOCAL_MODEL_PATH` [adeline/config.py88](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L88-L88) When `false`, the system requires a valid `ROBOFLOW_API_KEY` environment variable [adeline/config.py94-100](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L94-L100) and downloads the model specified by `MODEL_ID`.

**Image Size**: The `models.imgsz` parameter [adeline/config.py89](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L89-L89) sets the input resolution for the model. Smaller values (e.g., `320`) provide faster inference at the cost of accuracy. Larger values (e.g., `640`) improve detection quality but require more compute resources.

**Confidence Threshold**: Detections with confidence scores below `models.confidence` [adeline/config.py90](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L90-L90) are filtered out. A value of `0.25` means only detections with 25% or higher confidence are retained.

**IOU Threshold**: The `models.iou_threshold` [adeline/config.py91](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L91-L91) controls non-maximum suppression (NMS). Higher values allow more overlapping detections; lower values suppress redundant boxes more aggressively.

### Example Configuration

```
models:
  use_local: true
  local_path: "models/yolov11n-320.onnx"
  imgsz: 320
  confidence: 0.3
  iou_threshold: 0.5
```

**Sources:** [adeline/config.py86-91](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L86-L91) [adeline/config.py94-100](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L94-L100)

---

## ROI Strategy Configuration

The `roi_strategy` section enables dynamic or static Region of Interest processing. Three modes are supported: `none` (full frame), `adaptive` (dynamic cropping based on detections), and `fixed` (static coordinates).

### ROI Mode Selection

### Adaptive ROI Settings

When `roi_strategy.mode: adaptive` is set, the system dynamically adjusts the crop region based on recent detections. This reduces inference area when objects are localized.

|Configuration Key|Python Attribute|Type|Default|Description|
|---|---|---|---|---|
|`roi_strategy.adaptive.margin`|`CROP_MARGIN`|float|`0.2`|Expansion factor around detection bounding box|
|`roi_strategy.adaptive.smoothing`|`CROP_SMOOTHING`|float|`0.3`|Temporal smoothing weight (0=no smoothing, 1=max)|
|`roi_strategy.adaptive.min_roi_multiple`|`CROP_MIN_ROI_MULTIPLE`|int|`1`|Minimum ROI size as multiple of model input size|
|`roi_strategy.adaptive.max_roi_multiple`|`CROP_MAX_ROI_MULTIPLE`|int|`4`|Maximum ROI size as multiple of model input size|
|`roi_strategy.adaptive.show_statistics`|`CROP_SHOW_STATISTICS`|boolean|`true`|Display ROI size metrics on visualization|
|`roi_strategy.adaptive.resize_to_model`|`ADAPTIVE_RESIZE_TO_MODEL`|boolean|`false`|Resize cropped ROI to model input size|

**Margin**: The `margin` parameter [adeline/config.py161](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L161-L161) adds a buffer around detected objects. A value of `0.2` means the ROI extends 20% beyond the bounding box on all sides.

**Smoothing**: The `smoothing` parameter [adeline/config.py162](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L162-L162) applies exponential moving average to prevent jittery ROI adjustments. Higher values create smoother transitions but slower adaptation.

**ROI Size Constraints**: The `min_roi_multiple` and `max_roi_multiple` parameters [adeline/config.py163-164](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L163-L164) constrain the ROI size relative to the model's input size. This prevents extremely small or large crop regions.

### Fixed ROI Settings

When `roi_strategy.mode: fixed` is set, the system crops to a static rectangular region defined by normalized coordinates.

|Configuration Key|Python Attribute|Type|Default|Description|
|---|---|---|---|---|
|`roi_strategy.fixed.x_min`|`FIXED_X_MIN`|float|`0.2`|Left edge (0=left, 1=right)|
|`roi_strategy.fixed.y_min`|`FIXED_Y_MIN`|float|`0.2`|Top edge (0=top, 1=bottom)|
|`roi_strategy.fixed.x_max`|`FIXED_X_MAX`|float|`0.8`|Right edge (0=left, 1=right)|
|`roi_strategy.fixed.y_max`|`FIXED_Y_MAX`|float|`0.8`|Bottom edge (0=top, 1=bottom)|
|`roi_strategy.fixed.show_overlay`|`FIXED_SHOW_OVERLAY`|boolean|`true`|Draw ROI rectangle on visualization|
|`roi_strategy.fixed.resize_to_model`|`FIXED_RESIZE_TO_MODEL`|boolean|`false`|Resize cropped ROI to model input size|

Coordinates are normalized to the frame dimensions, where `(0, 0)` is the top-left corner and `(1, 1)` is the bottom-right corner.

### Example Configuration

```
# Adaptive ROI example
roi_strategy:
  mode: adaptive
  adaptive:
    margin: 0.25
    smoothing: 0.4
    min_roi_multiple: 1
    max_roi_multiple: 3
    show_statistics: true
    resize_to_model: false

# Fixed ROI example
roi_strategy:
  mode: fixed
  fixed:
    x_min: 0.1
    y_min: 0.1
    x_max: 0.9
    y_max: 0.9
    show_overlay: true
    resize_to_model: false

# No ROI example
roi_strategy:
  mode: none
```

### Legacy Configuration Support

The system supports backward compatibility with the older `adaptive_crop.enabled` configuration format [adeline/config.py178-205](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L178-L205) If `roi_strategy` is not present, the system checks for `adaptive_crop.enabled` and converts it to the new format:

- `adaptive_crop.enabled: true` → `roi_strategy.mode: adaptive`
- `adaptive_crop.enabled: false` → `roi_strategy.mode: none`

A warning is logged when legacy configuration is detected [adeline/config.py201-205](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L201-L205)

**Sources:** [adeline/config.py152-200](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L152-L200)

---

## Detection Stabilization Configuration

The `detection_stabilization` section controls temporal filtering to reduce flickering detections. The stabilization layer requires objects to appear consistently across multiple frames before being reported.

### Settings Reference

|Configuration Key|Python Attribute|Type|Default|Description|
|---|---|---|---|---|
|`detection_stabilization.mode`|`STABILIZATION_MODE`|string|`none`|Stabilization strategy (`none`, `temporal`)|
|`detection_stabilization.temporal.min_frames`|`STABILIZATION_MIN_FRAMES`|int|`3`|Consecutive frames required for confirmation|
|`detection_stabilization.temporal.max_gap`|`STABILIZATION_MAX_GAP`|int|`2`|Maximum gap frames before resetting track|
|`detection_stabilization.hysteresis.appear_confidence`|`STABILIZATION_APPEAR_CONF`|float|`0.5`|Confidence threshold to appear|
|`detection_stabilization.hysteresis.persist_confidence`|`STABILIZATION_PERSIST_CONF`|float|`0.3`|Confidence threshold to persist|

### Temporal + Hysteresis Strategy

**Temporal Filtering**: The `temporal.min_frames` parameter [adeline/config.py141](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L141-L141) requires a detection to appear in N consecutive frames before being confirmed. This eliminates transient false positives.

**Gap Tolerance**: The `temporal.max_gap` parameter [adeline/config.py142](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L142-L142) allows up to N frames without detection before a track is dropped. This handles brief occlusions.

**Hysteresis Thresholds**: The hysteresis mechanism uses two confidence thresholds [adeline/config.py146-147](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L146-L147):

- `appear_confidence`: Higher threshold required for a new detection to start tracking
- `persist_confidence`: Lower threshold allows existing tracks to continue

This prevents flickering by making it harder to appear than to persist, similar to a Schmitt trigger in electronics.

### Example Configuration

```
detection_stabilization:
  mode: temporal
  temporal:
    min_frames: 3
    max_gap: 2
  hysteresis:
    appear_confidence: 0.5
    persist_confidence: 0.3
```

**Sources:** [adeline/config.py136-147](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L136-L147)

---

## Logging Configuration

The `logging` section controls log output verbosity and formatting for the application and MQTT client library.

### Settings Reference

|Configuration Key|Python Attribute|Type|Default|Description|
|---|---|---|---|---|
|`logging.level`|`LOG_LEVEL`|string|`INFO`|Application log level (DEBUG, INFO, WARNING, ERROR)|
|`logging.format`|`LOG_FORMAT`|string|`%(asctime)s - %(name)s - %(levelname)s - %(message)s`|Python logging format string|
|`logging.paho_level`|`PAHO_LOG_LEVEL`|string|`WARNING`|MQTT client log level (separate from main log)|

**Application Logging**: The `logging.level` parameter [adeline/config.py126](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L126-L126) sets the verbosity for all Adeline components. `DEBUG` produces extensive output; `INFO` shows key events; `WARNING` and `ERROR` show only problems.

**Format String**: The `logging.format` parameter [adeline/config.py127](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L127-L127) defines the log message structure using Python's logging format syntax.

**MQTT Client Logging**: The `logging.paho_level` parameter [adeline/config.py128](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L128-L128) separately controls the paho-mqtt library's log output. This is typically set to `WARNING` to reduce noise from connection/subscription events.

### Example Configuration

```
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  paho_level: "WARNING"
```

**Sources:** [adeline/config.py125-128](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L125-L128)

---

## Complete Configuration Example

Below is a comprehensive example showing all pipeline settings in a single `config.yaml` file:

```
# Pipeline core settings
pipeline:
  rtsp_url: "rtsp://127.0.0.1:8554/live"
  model_id: "yolov11n-640"
  max_fps: 2
  enable_visualization: true
  display_statistics: true

# Model inference configuration
models:
  use_local: false
  local_path: "models/yolov11n-320.onnx"
  imgsz: 320
  confidence: 0.25
  iou_threshold: 0.45

# ROI strategy
roi_strategy:
  mode: adaptive  # Options: none, adaptive, fixed
  adaptive:
    margin: 0.2
    smoothing: 0.3
    min_roi_multiple: 1
    max_roi_multiple: 4
    show_statistics: true
    resize_to_model: false
  fixed:
    x_min: 0.2
    y_min: 0.2
    x_max: 0.8
    y_max: 0.8
    show_overlay: true
    resize_to_model: false

# Detection stabilization
detection_stabilization:
  mode: temporal  # Options: none, temporal
  temporal:
    min_frames: 3
    max_gap: 2
  hysteresis:
    appear_confidence: 0.5
    persist_confidence: 0.3

# Logging configuration
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  paho_level: "WARNING"
```

**Sources:** [adeline/config.py78-206](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L78-L206)

---

## Configuration Validation

The `PipelineConfig` constructor performs validation during initialization:

1. **File Existence**: Raises `FileNotFoundError` if `config.yaml` is missing [adeline/config.py68-72](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L68-L72)
2. **API Key Check**: When `models.use_local: false`, validates that `ROBOFLOW_API_KEY` environment variable exists [adeline/config.py95-100](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L95-L100)
3. **Type Conversion**: YAML values are automatically converted to Python types (strings, integers, floats, booleans)
4. **Default Values**: All settings have default values if not specified in the configuration file

Any configuration errors halt system initialization before the inference pipeline starts, ensuring consistent operation.

**Sources:** [adeline/config.py59-100](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L59-L100)