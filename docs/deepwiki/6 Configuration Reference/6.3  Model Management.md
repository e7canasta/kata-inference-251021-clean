# Model Management

Relevant source files

- [adeline/CLAUDE.md](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/CLAUDE.md)
- [adeline/DESIGN.md](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/DESIGN.md)
- [adeline/config.py](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py)

## Purpose and Scope

This document explains how the Adeline inference system loads, configures, and manages machine learning models. It covers the critical initialization pattern that prevents dependency warnings, the choice between local ONNX models and Roboflow Cloud API, and the configuration structure for model settings.

For information about the inference pipeline that uses these models, see [Inference Pipeline](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/5.2-inference-pipeline). For general configuration hierarchy, see [Configuration Hierarchy](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/6.1-configuration-hierarchy).

---

## Overview

The Adeline system supports two model loading mechanisms:

1. **Local ONNX Models**: Load pre-downloaded ONNX model files from the filesystem
2. **Roboflow Cloud API**: Download and use models from Roboflow's cloud service

A critical aspect of model management is the **explicit model disabling pattern**, which prevents the `inference` library from loading heavy models that are not needed by the pipeline. This pattern requires careful initialization order to avoid `ModelDependencyMissing` warnings.

**Key Components**:

- `disable_models_from_config()` function in [adeline/config.py22-51](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L22-L51)
- `PipelineConfig.models` section in [adeline/config.py85-100](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L85-L100)
- `models_disabled.disabled` array in `config.yaml`

---

## Model Loading Mechanisms

### Local vs Roboflow API Models

The system determines which model loading mechanism to use based on the `models.use_local` configuration flag.

**Sources**: [adeline/config.py85-100](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L85-L100) [adeline/CLAUDE.md71-73](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/CLAUDE.md#L71-L73)

### Configuration Attributes

The `PipelineConfig` class exposes these model-related attributes:

|Attribute|Config Key|Default|Description|
|---|---|---|---|
|`USE_LOCAL_MODEL`|`models.use_local`|`False`|Use local ONNX files instead of Roboflow API|
|`LOCAL_MODEL_PATH`|`models.local_path`|`models/yolov11n-320.onnx`|Path to local ONNX model file|
|`MODEL_IMGSZ`|`models.imgsz`|`320`|Model input image size|
|`MODEL_CONFIDENCE`|`models.confidence`|`0.25`|Confidence threshold for detections|
|`MODEL_IOU_THRESHOLD`|`models.iou_threshold`|`0.45`|IoU threshold for NMS|
|`API_KEY`|`ROBOFLOW_API_KEY` (env)|None|Roboflow API key (required if `use_local=False`)|
|`MODEL_ID`|`pipeline.model_id`|`yolov11n-640`|Model identifier for Roboflow API|

**Sources**: [adeline/config.py85-100](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L85-L100)

### API Key Validation

When using Roboflow Cloud API (`use_local=False`), the system validates that the `ROBOFLOW_API_KEY` environment variable is set. If missing, initialization fails with a descriptive error:

```
ValueError: ROBOFLOW_API_KEY not found in environment variables.
Please set it in your .env file (copy from .env.example)
Or set models.use_local: true to use local ONNX models
```

This validation happens during `PipelineConfig.__init__()` at [adeline/config.py94-100](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L94-L100)

**Sources**: [adeline/config.py94-100](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L94-L100)

---

## Critical Initialization Pattern: disable_models_from_config()

### Why This Pattern Exists

The `inference` library from Roboflow loads many heavy models during import, including:

- PALIGEMMA, FLORENCE2, QWEN_2_5 (vision-language models)
- SAM, SAM2, CLIP (segmentation and embedding models)
- GroundingDINO, YOLO-World (grounding models)
- Many others

If these models' dependencies are not installed, the library emits `ModelDependencyMissing` warnings. Since Adeline only uses YOLO for object detection, these other models are unnecessary and should be explicitly disabled.

**The Critical Rule**: `disable_models_from_config()` **MUST** be called **BEFORE** importing any module from the `inference` package.

**Sources**: [adeline/config.py22-28](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L22-L28) [adeline/CLAUDE.md46-49](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/CLAUDE.md#L46-L49) [adeline/DESIGN.md51-62](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/DESIGN.md#L51-L62)

### How disable_models_from_config() Works

The function reads the `models_disabled.disabled` array from `config.yaml` and sets environment variables to disable each listed model:

**Sources**: [adeline/config.py22-51](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L22-L51)

### Default Disabled Models

If `config.yaml` does not exist, the function disables these 13 heavy models by default:

```
default_disabled = [
    "PALIGEMMA", "FLORENCE2", "QWEN_2_5",
    "CORE_MODEL_SAM", "CORE_MODEL_SAM2", "CORE_MODEL_CLIP",
    "CORE_MODEL_GAZE", "SMOLVLM2", "DEPTH_ESTIMATION",
    "MOONDREAM2", "CORE_MODEL_TROCR", "CORE_MODEL_GROUNDINGDINO",
    "CORE_MODEL_YOLO_WORLD", "CORE_MODEL_PE",
]
```

These are defined at [adeline/config.py32-38](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L32-L38)

**Sources**: [adeline/config.py32-38](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L32-L38)

### Configuration Structure

In `config.yaml`, models are disabled using this structure:

```
models_disabled:
  disabled:
    - PALIGEMMA
    - FLORENCE2
    - QWEN_2_5
    - CORE_MODEL_SAM
    - CORE_MODEL_SAM2
    - CORE_MODEL_CLIP
    # ... additional models
```

The function reads `models_disabled.disabled` at [adeline/config.py46-47](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L46-L47) and iterates through the list to set environment variables.

**Sources**: [adeline/config.py43-50](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L43-L50)

---

## Initialization Sequence

The following sequence diagram shows the **critical order** of operations during system initialization. This order is enforced to prevent model loading issues.

### Initialization Flow with Code References

**Sources**: [adeline/config.py22-51](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L22-L51) [adeline/CLAUDE.md44-54](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/CLAUDE.md#L44-L54) [adeline/DESIGN.md51-62](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/DESIGN.md#L51-L62)

### Enforcement in Code

The initialization order is enforced at the application entry point. In `__main__.py`, the pattern is:

```
# STEP 1: Disable models BEFORE any imports
from adeline.config import disable_models_from_config
disable_models_from_config()

# STEP 2: NOW safe to import inference
from adeline.app import main

# STEP 3: Run application
main()
```

This pattern is documented at [adeline/CLAUDE.md106-109](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/CLAUDE.md#L106-L109) and [adeline/DESIGN.md54-61](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/DESIGN.md#L54-L61)

**Sources**: [adeline/CLAUDE.md106-109](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/CLAUDE.md#L106-L109) [adeline/DESIGN.md54-61](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/DESIGN.md#L54-L61)

---

## Model Configuration Reference

### Complete Configuration Example

```
# Pipeline settings
pipeline:
  model_id: yolov11n-640  # Roboflow API model identifier
  rtsp_url: rtsp://127.0.0.1:8554/live
  max_fps: 2

# Model loading and inference settings
models:
  use_local: false  # true = local ONNX, false = Roboflow API
  local_path: models/yolov11n-320.onnx  # Path to ONNX file (if use_local=true)
  imgsz: 320  # Model input size
  confidence: 0.25  # Detection confidence threshold
  iou_threshold: 0.45  # NMS IoU threshold

# Models to disable (prevent loading during import)
models_disabled:
  disabled:
    - PALIGEMMA
    - FLORENCE2
    - QWEN_2_5
    - CORE_MODEL_SAM
    - CORE_MODEL_SAM2
    - CORE_MODEL_CLIP
    - CORE_MODEL_GAZE
    - SMOLVLM2
    - DEPTH_ESTIMATION
    - MOONDREAM2
    - CORE_MODEL_TROCR
    - CORE_MODEL_GROUNDINGDINO
    - CORE_MODEL_YOLO_WORLD
    - CORE_MODEL_PE
```

**Sources**: [adeline/config.py78-100](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L78-L100) [adeline/config.py130-131](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L130-L131)

### Environment Variables

|Variable|Required When|Description|
|---|---|---|
|`ROBOFLOW_API_KEY`|`use_local=False`|API key for Roboflow Cloud service|
|`{MODEL}_ENABLED`|Set automatically|Environment flags set by `disable_models_from_config()`|

The `{MODEL}_ENABLED` variables (e.g., `PALIGEMMA_ENABLED=False`) are set programmatically by `disable_models_from_config()` and should not be manually configured in `.env`.

**Sources**: [adeline/config.py94-100](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L94-L100) [adeline/config.py40-50](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L40-L50)

---

## Model Loading Decision Tree

The following diagram shows how the system decides which models to load and how:

**Sources**: [adeline/config.py22-100](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L22-L100) [adeline/CLAUDE.md44-54](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/CLAUDE.md#L44-L54) [adeline/CLAUDE.md71-73](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/CLAUDE.md#L71-L73)

---

## Common Patterns and Best Practices

### Pattern 1: Always Disable Unused Models

Always maintain a comprehensive `models_disabled.disabled` list in `config.yaml` to prevent unnecessary model loading attempts:

```
models_disabled:
  disabled:
    - PALIGEMMA
    - FLORENCE2
    # ... all unused models
```

This prevents warnings and reduces memory footprint during initialization.

**Sources**: [adeline/config.py43-50](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L43-L50) [adeline/CLAUDE.md106-109](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/CLAUDE.md#L106-L109)

### Pattern 2: Local Development with ONNX Models

For local development without internet access or Roboflow API quota concerns:

```
models:
  use_local: true
  local_path: models/yolov11n-320.onnx
  imgsz: 320
```

Ensure the ONNX file exists at the specified path before starting the pipeline.

**Sources**: [adeline/config.py87-89](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L87-L89) [adeline/CLAUDE.md71-73](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/CLAUDE.md#L71-L73)

### Pattern 3: Production with Roboflow API

For production deployments using Roboflow's model management:

```
models:
  use_local: false

pipeline:
  model_id: yolov11n-640
```

And in `.env`:

```
ROBOFLOW_API_KEY=your_api_key_here
```

**Sources**: [adeline/config.py94-100](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L94-L100)

---

## Troubleshooting

### ModelDependencyMissing Warnings

**Symptom**: Console output shows warnings about missing model dependencies during import.

**Cause**: `disable_models_from_config()` was not called before importing `inference` module, or the `models_disabled.disabled` list is incomplete.

**Solution**:

1. Verify that `disable_models_from_config()` is called in `__main__.py` before any `inference` imports
2. Add missing models to the `models_disabled.disabled` array in `config.yaml`

**Sources**: [adeline/config.py22-28](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L22-L28) [adeline/CLAUDE.md46-49](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/CLAUDE.md#L46-L49)

### ROBOFLOW_API_KEY Not Found

**Symptom**: `ValueError: ROBOFLOW_API_KEY not found in environment variables`

**Cause**: Using Roboflow API mode (`use_local=False`) without setting the API key.

**Solution**:

1. Copy `.env.example` to `.env`
2. Set `ROBOFLOW_API_KEY=your_key` in `.env`
3. Or switch to local mode: `models.use_local: true`

**Sources**: [adeline/config.py94-100](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L94-L100)

### Local Model File Not Found

**Symptom**: Pipeline fails to initialize with file not found error.

**Cause**: `LOCAL_MODEL_PATH` points to non-existent ONNX file.

**Solution**:

1. Download or export the ONNX model file
2. Place it at the path specified in `models.local_path`
3. Or update `models.local_path` to match the actual file location

**Sources**: [adeline/config.py87-89](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L87-L89)

---

## Summary

Model management in Adeline follows these key principles:

1. **Explicit Model Disabling**: Use `disable_models_from_config()` before importing `inference` to prevent loading unused heavy models
2. **Critical Initialization Order**: Models must be disabled before any `inference` imports occur
3. **Flexible Model Sources**: Support both local ONNX files and Roboflow Cloud API
4. **Configuration-Driven**: All model settings are controlled via `config.yaml` and `.env`, not hardcoded
5. **Fail-Fast Validation**: Missing API keys or invalid configurations are caught during initialization

This design ensures efficient resource usage while maintaining flexibility for different deployment scenarios.

**Sources**: [adeline/config.py22-100](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L22-L100) [adeline/CLAUDE.md44-54](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/CLAUDE.md#L44-L54) [adeline/CLAUDE.md71-73](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/CLAUDE.md#L71-L73) [adeline/CLAUDE.md106-109](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/CLAUDE.md#L106-L109) [adeline/DESIGN.md51-62](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/DESIGN.md#L51-L62)