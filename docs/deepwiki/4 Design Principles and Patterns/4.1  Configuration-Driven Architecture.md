# Configuration-Driven Architecture

Relevant source files

- [adeline/DESIGN.md](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/DESIGN.md)
- [adeline/config.py](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py)

## Purpose and Scope

This page documents the configuration-driven architecture pattern used throughout the Adeline inference system. Configuration-driven architecture means that system behavior is controlled through external configuration files rather than hardcoded logic, enabling flexible deployment across different environments without code modification or recompilation.

This page covers the three-tier configuration hierarchy, the `PipelineConfig` class, configuration loading and validation, and the critical `disable_models_from_config()` pattern. For information about how configuration drives strategy selection at runtime, see [Factory Pattern for Strategies](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/4.2-factory-pattern-for-strategies). For MQTT-specific configuration details, see [MQTT Configuration](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/6.4-mqtt-configuration).

---

## The Three-Tier Configuration System

The Adeline system uses a three-tier configuration hierarchy that separates concerns by sensitivity and purpose:

|Tier|File(s)|Purpose|Contents|
|---|---|---|---|
|**Secrets**|`.env`|Sensitive credentials|API keys, MQTT credentials, passwords|
|**Settings**|`config.yaml`|Pipeline configuration|Model selection, ROI strategy, stabilization mode, thresholds|
|**Infrastructure**|`docker-compose.yml`|Service definitions|MQTT broker, go2rtc proxy, port mappings|

**Sources:** [adeline/config.py1-206](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L1-L206) [adeline/DESIGN.md38-49](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/DESIGN.md#L38-L49)

### Tier 1: Environment Variables (.env)

The `.env` file stores sensitive credentials that should never be committed to version control:

```
ROBOFLOW_API_KEY=your_api_key_here
MQTT_USERNAME=your_username
MQTT_PASSWORD=your_password
```

These values are loaded using `python-dotenv` and accessed via `os.getenv()`.

**Sources:** [adeline/config.py14](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L14-L14) [adeline/config.py94-100](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L94-L100) [adeline/config.py109-110](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L109-L110)

### Tier 2: Pipeline Settings (config.yaml)

The `config.yaml` file defines all pipeline behavior, including:

- RTSP stream URLs
- Model selection and parameters
- ROI strategy mode and parameters
- Detection stabilization settings
- MQTT topics and QoS levels
- Logging configuration

**Sources:** [adeline/config.py59-206](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L59-L206)

### Tier 3: Infrastructure Services (docker-compose.yml)

Docker Compose files define infrastructure services like the MQTT broker and RTSP proxy, separating deployment configuration from application logic.

**Sources:** High-level diagrams (Diagram 2)

---

## Configuration Loading Process

### The PipelineConfig Class

The `PipelineConfig` class is the single source of truth for all pipeline configuration. It loads and validates configuration from both `config.yaml` and environment variables.

**Configuration Loading Flow: From Files to PipelineConfig Class**

**Sources:** [adeline/config.py56-206](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L56-L206)

### Critical Initialization: disable_models_from_config()

Before the `inference` module can be imported, `disable_models_from_config()` must be called to prevent loading heavy models that are not needed. This function reads the `models_disabled.disabled[]` array from `config.yaml` and sets environment variables that the inference module checks during import.

**Critical Initialization Sequence: Model Disabling Before Import**

This pattern makes the implicit side effect of model loading during import explicit and controllable through configuration.

**Sources:** [adeline/config.py22-51](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L22-L51) [adeline/DESIGN.md52-61](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/DESIGN.md#L52-L61)

---

## Configuration Schema Structure

The `PipelineConfig` class defines the following configuration schema:

### Pipeline Configuration

|Attribute|Config Path|Default|Description|
|---|---|---|---|
|`RTSP_URL`|`pipeline.rtsp_url`|`rtsp://127.0.0.1:8554/live`|RTSP stream source|
|`MODEL_ID`|`pipeline.model_id`|`yolov11n-640`|Roboflow model identifier|
|`MAX_FPS`|`pipeline.max_fps`|`2`|Maximum inference frames per second|
|`ENABLE_VISUALIZATION`|`pipeline.enable_visualization`|`True`|Enable OpenCV display|
|`DISPLAY_STATISTICS`|`pipeline.display_statistics`|`True`|Show performance stats|

**Sources:** [adeline/config.py78-83](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L78-L83)

### Model Configuration

|Attribute|Config Path|Default|Description|
|---|---|---|---|
|`USE_LOCAL_MODEL`|`models.use_local`|`False`|Use local ONNX vs Roboflow API|
|`LOCAL_MODEL_PATH`|`models.local_path`|`models/yolov11n-320.onnx`|Path to local ONNX file|
|`MODEL_IMGSZ`|`models.imgsz`|`320`|Model input image size|
|`MODEL_CONFIDENCE`|`models.confidence`|`0.25`|Detection confidence threshold|
|`MODEL_IOU_THRESHOLD`|`models.iou_threshold`|`0.45`|NMS IoU threshold|
|`API_KEY`|`ROBOFLOW_API_KEY` (env)|N/A|Roboflow API key (required if not local)|

**Sources:** [adeline/config.py86-100](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L86-L100)

### MQTT Configuration

|Attribute|Config Path|Default|Description|
|---|---|---|---|
|`MQTT_BROKER`|`mqtt.broker.host`|`localhost`|MQTT broker hostname|
|`MQTT_PORT`|`mqtt.broker.port`|`1883`|MQTT broker port|
|`MQTT_USERNAME`|`MQTT_USERNAME` (env)|N/A|MQTT authentication username|
|`MQTT_PASSWORD`|`MQTT_PASSWORD` (env)|N/A|MQTT authentication password|
|`CONTROL_COMMAND_TOPIC`|`mqtt.topics.control_commands`|`inference/control/commands`|Control plane command topic|
|`CONTROL_STATUS_TOPIC`|`mqtt.topics.control_status`|`inference/control/status`|Control plane status topic|
|`DATA_TOPIC`|`mqtt.topics.data`|`inference/data/detections`|Data plane detection topic|
|`METRICS_TOPIC`|`mqtt.topics.metrics`|`inference/data/metrics`|Data plane metrics topic|
|`CONTROL_QOS`|`mqtt.qos.control`|`1`|Control plane QoS level (reliable)|
|`DATA_QOS`|`mqtt.qos.data`|`0`|Data plane QoS level (performance)|

**Sources:** [adeline/config.py103-122](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L103-L122)

### ROI Strategy Configuration

|Attribute|Config Path|Default|Description|
|---|---|---|---|
|`ROI_MODE`|`roi_strategy.mode`|`none`|ROI strategy: `none`, `adaptive`, or `fixed`|
|`CROP_MARGIN`|`roi_strategy.adaptive.margin`|`0.2`|Adaptive ROI margin (20% padding)|
|`CROP_SMOOTHING`|`roi_strategy.adaptive.smoothing`|`0.3`|Temporal smoothing factor|
|`FIXED_X_MIN`|`roi_strategy.fixed.x_min`|`0.2`|Fixed ROI normalized x_min|
|`FIXED_Y_MIN`|`roi_strategy.fixed.y_min`|`0.2`|Fixed ROI normalized y_min|
|`FIXED_X_MAX`|`roi_strategy.fixed.x_max`|`0.8`|Fixed ROI normalized x_max|
|`FIXED_Y_MAX`|`roi_strategy.fixed.y_max`|`0.8`|Fixed ROI normalized y_max|

**Sources:** [adeline/config.py150-200](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L150-L200)

### Detection Stabilization Configuration

|Attribute|Config Path|Default|Description|
|---|---|---|---|
|`STABILIZATION_MODE`|`detection_stabilization.mode`|`none`|Stabilization strategy: `none` or `temporal`|
|`STABILIZATION_MIN_FRAMES`|`detection_stabilization.temporal.min_frames`|`3`|Min consecutive frames to appear|
|`STABILIZATION_MAX_GAP`|`detection_stabilization.temporal.max_gap`|`2`|Max frame gap before disappearing|
|`STABILIZATION_APPEAR_CONF`|`detection_stabilization.hysteresis.appear_confidence`|`0.5`|High threshold for new detections|
|`STABILIZATION_PERSIST_CONF`|`detection_stabilization.hysteresis.persist_confidence`|`0.3`|Low threshold for persisting detections|

**Sources:** [adeline/config.py135-148](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L135-L148)

---

## Configuration Flow Through the System

**Configuration Flow: From Files to Runtime Components**

The configuration flows from files through the `PipelineConfig` class to the controller, which uses it to initialize factories and components. The factories use configuration to determine which concrete strategy classes to instantiate.

**Sources:** [adeline/config.py1-206](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L1-L206) [adeline/DESIGN.md38-49](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/DESIGN.md#L38-L49)

---

## Benefits of Configuration-Driven Architecture

### 1. Flexibility Without Recompilation

System behavior can be changed by editing `config.yaml` without modifying or recompiling code. For example, switching from adaptive ROI to fixed ROI requires only changing:

```
roi_strategy:
  mode: fixed  # Changed from "adaptive"
```

**Sources:** [adeline/DESIGN.md38-49](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/DESIGN.md#L38-L49)

### 2. Environment-Specific Deployment

The same codebase can be deployed to different environments (development, staging, production) with different configurations:

- **Development**: Use local ONNX models, visualization enabled, low FPS
- **Production**: Use Roboflow API, visualization disabled, high FPS

**Sources:** [adeline/config.py86-100](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L86-L100)

### 3. Separation of Concerns

Configuration files have clear boundaries:

|Concern|File|Reason|
|---|---|---|
|Secrets|`.env`|Never committed, environment-specific|
|Business Logic|`config.yaml`|Version-controlled, reviewed|
|Infrastructure|`docker-compose.yml`|Separate from application logic|

**Sources:** [adeline/DESIGN.md38-49](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/DESIGN.md#L38-L49)

### 4. Explicit Over Implicit

The `disable_models_from_config()` pattern makes model loading explicit and controllable. Without this pattern, importing the `inference` module would have hidden side effects (loading heavy models), making debugging difficult.

```
# BEFORE: Implicit, uncontrollable
from inference import InferencePipeline  # Loads ALL models 😱

# AFTER: Explicit, controllable
disable_models_from_config()  # Based on config
from inference import InferencePipeline  # Only loads enabled models ✅
```

**Sources:** [adeline/config.py22-51](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L22-L51) [adeline/DESIGN.md52-61](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/DESIGN.md#L52-L61)

### 5. Runtime Behavior Control

Configuration determines runtime behavior through factory pattern selection:

```
# Change ROI strategy without code changes
roi_strategy:
  mode: adaptive  # or "fixed" or "none"

# Change stabilization without code changes
detection_stabilization:
  mode: temporal  # or "none"
```

The factories use the `mode` string from configuration to instantiate the appropriate strategy class.

**Sources:** [adeline/DESIGN.md24-36](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/DESIGN.md#L24-L36) [adeline/config.py137](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L137-L137) [adeline/config.py156](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L156-L156)

---

## Validation and Error Handling

The `PipelineConfig` class performs validation during initialization:

### Required File Validation

```
# adeline/config.py:68-72
if not config_file.exists():
    raise FileNotFoundError(
        f"Config file not found: {config_path}\n"
        f"Please create it from config/adeline/config.yaml.example"
    )
```

**Sources:** [adeline/config.py68-72](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L68-L72)

### API Key Validation

```
# adeline/config.py:94-100
if not self.USE_LOCAL_MODEL and not self.API_KEY:
    raise ValueError(
        "ROBOFLOW_API_KEY not found in environment variables.\n"
        "Please set it in your .env file (copy from .env.example)\n"
        "Or set models.use_local: true to use local ONNX models"
    )
```

This validation ensures that if Roboflow models are used, the API key is present.

**Sources:** [adeline/config.py94-100](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L94-L100)

### Backward Compatibility

The configuration system supports legacy configuration formats to prevent breaking existing deployments:

```
# adeline/config.py:178-205
# Backward compatibility: Legacy adaptive_crop.enabled structure
adaptive_crop_cfg = config.get('adaptive_crop', {})
legacy_enabled = adaptive_crop_cfg.get('enabled', False)

# Convert legacy config to new ROI_MODE
self.ROI_MODE = 'adaptive' if legacy_enabled else 'none'
```

**Sources:** [adeline/config.py178-205](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L178-L205)

---

## Anti-Patterns Avoided

The configuration-driven architecture explicitly avoids these anti-patterns:

### ❌ Hardcoded Logic Scattered Across Files

```
# BAD: Hardcoded in multiple places
if mode == "adaptive":  # Repeated in 10 different files
    # adaptive logic
```

Instead, mode is read from configuration once and used to select strategies via factories.

**Sources:** [adeline/DESIGN.md84-88](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/DESIGN.md#L84-L88)

### ❌ Magic Side Effects During Import

```
# BAD: Importing has hidden side effects
import inference  # Secretly loads 5GB of models
```

Instead, `disable_models_from_config()` is called explicitly before import.

**Sources:** [adeline/DESIGN.md88](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/DESIGN.md#L88-L88) [adeline/config.py22-51](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L22-L51)

### ❌ Tight Coupling to Specific Values

```
# BAD: System only works with these exact values
MQTT_BROKER = "localhost"  # Can't deploy elsewhere
```

Instead, all values are configurable through `config.yaml` or `.env`.

**Sources:** [adeline/config.py105-110](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L105-L110)

---

## Summary

The configuration-driven architecture achieves **complexity controlled through design** rather than complicated code:

1. **Three-tier hierarchy** separates secrets, settings, and infrastructure
2. **Single source of truth** (`PipelineConfig`) loads and validates all configuration
3. **Critical initialization order** (`disable_models_from_config()` before import) prevents unwanted side effects
4. **Factory pattern integration** enables configuration to select concrete implementations
5. **Validation on startup** catches configuration errors early with helpful messages
6. **Backward compatibility** prevents breaking existing deployments

The result is a system where behavior can be changed through configuration files without modifying code, enabling flexible deployment across different environments while maintaining clear separation of concerns.

**Sources:** [adeline/DESIGN.md1-117](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/DESIGN.md#L1-L117) [adeline/config.py1-206](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L1-L206)