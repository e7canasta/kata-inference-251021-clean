# Component Overview

Relevant source files

- [adeline/CLAUDE.md](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/CLAUDE.md)
- [adeline/__init__.py](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/__init__.py)
- [adeline/app/controller.py](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py)

This document describes the major components of the Adeline inference system and their interactions. It provides a comprehensive inventory of system components, their responsibilities, and how they work together to deliver real-time object detection with MQTT-based control.

For architectural patterns and design rationale, see [Control and Data Plane Separation](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/3.1-control-and-data-plane-separation). For MQTT-specific communication details, see [MQTT Communication Architecture](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/3.2-mqtt-communication-architecture). For the startup process, see [Initialization Sequence](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/3.4-initialization-sequence).

## Component Inventory

The Adeline system consists of seven primary components that work together to provide controlled, observable inference:

|Component|File Location|Primary Class|Purpose|
|---|---|---|---|
|Pipeline Controller|[adeline/app/controller.py](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py)|`InferencePipelineController`|Orchestrates all components, manages lifecycle, handles signals|
|Control Plane|[adeline/control/plane.py](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/control/plane.py)|`MQTTControlPlane`|Receives and processes control commands via MQTT (QoS 1)|
|Data Plane|[adeline/data/plane.py](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/data/plane.py)|`MQTTDataPlane`|Publishes inference results and metrics via MQTT (QoS 0)|
|Inference Pipeline|inference library|`InferencePipeline`|Executes YOLO object detection on video frames|
|ROI Strategy|[adeline/inference/roi/](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/inference/roi/)|`AdaptiveROIState`, `FixedROIState`|Manages region of interest cropping (adaptive/fixed/none)|
|Stabilization Strategy|[adeline/inference/stabilization/](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/inference/stabilization/)|`TemporalHysteresisStabilizer`|Filters detections to reduce flickering|
|Watchdog|inference library|`BasePipelineWatchDog`|Monitors pipeline performance metrics (FPS, latency)|

Sources: [adeline/__init__.py1-45](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/__init__.py#L1-L45) [adeline/CLAUDE.md25-93](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/CLAUDE.md#L25-L93)

## System Component Topology

This diagram maps the natural language component names to their concrete code entities (classes, functions, files). Each component is labeled with its actual implementation location, enabling direct code navigation.

Sources: [adeline/app/controller.py54-528](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L54-L528) [adeline/__init__.py27-44](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/__init__.py#L27-L44)

## InferencePipelineController

The `InferencePipelineController` class [adeline/app/controller.py54-528](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L54-L528) serves as the central orchestrator. It is responsible for:

- **Component lifecycle management**: Creates and initializes all other components in the correct order
- **Signal handling**: Registers handlers for `SIGINT` and `SIGTERM` to ensure graceful shutdown [adeline/app/controller.py462-463](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L462-L463)
- **Callback wiring**: Connects Control Plane callbacks to controller methods [adeline/app/controller.py290-301](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L290-L301)
- **Pipeline configuration**: Determines whether to use standard pipeline or custom logic based on ROI mode [adeline/app/controller.py131-276](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L131-L276)
- **Resource cleanup**: Ensures proper disconnection of MQTT clients and pipeline termination [adeline/app/controller.py489-527](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L489-L527)

### Key Methods

|Method|Line Reference|Purpose|
|---|---|---|
|`__init__()`|[adeline/app/controller.py59-66](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L59-L66)|Initializes instance variables, creates shutdown event|
|`setup()`|[adeline/app/controller.py68-319](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L68-L319)|Creates and connects all components, starts pipeline|
|`run()`|[adeline/app/controller.py436-474](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L436-L474)|Main execution loop, waits on shutdown event|
|`cleanup()`|[adeline/app/controller.py489-527](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L489-L527)|Terminates pipeline, disconnects MQTT, exits process|
|`_handle_stop()`|[adeline/app/controller.py328-341](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L328-L341)|Stops pipeline and sets shutdown event|
|`_handle_pause()`|[adeline/app/controller.py343-353](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L343-L353)|Pauses video stream processing|
|`_handle_resume()`|[adeline/app/controller.py355-365](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L355-L365)|Resumes video stream processing|
|`_handle_metrics()`|[adeline/app/controller.py367-373](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L367-L373)|Triggers metric publication via Data Plane|
|`_handle_toggle_crop()`|[adeline/app/controller.py375-397](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L375-L397)|Enables/disables adaptive ROI cropping|
|`_handle_stabilization_stats()`|[adeline/app/controller.py399-434](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L399-L434)|Logs and reports stabilization statistics|

### State Management

The controller maintains critical state flags:

- `self.is_running`: Boolean tracking whether pipeline is actively processing [adeline/app/controller.py66](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L66-L66)
- `self.shutdown_event`: Threading event for coordinating graceful shutdown [adeline/app/controller.py65](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L65-L65)
- `self.pipeline`: Reference to `InferencePipeline` instance [adeline/app/controller.py61](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L61-L61)
- `self.control_plane`: Reference to `MQTTControlPlane` instance [adeline/app/controller.py62](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L62-L62)
- `self.data_plane`: Reference to `MQTTDataPlane` instance [adeline/app/controller.py63](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L63-L63)
- `self.watchdog`: Reference to `BasePipelineWatchDog` instance [adeline/app/controller.py64](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L64-L64)

Sources: [adeline/app/controller.py54-528](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L54-L528)

## MQTTControlPlane

The `MQTTControlPlane` component [adeline/control/plane.py](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/control/plane.py) handles reliable command reception with QoS 1 guarantees. It provides a callback-based interface for pipeline control.

### Responsibilities

1. **Command reception**: Subscribes to `inference/control/commands` topic and parses JSON commands
2. **Callback invocation**: Executes registered callback functions when commands are received
3. **Status publishing**: Publishes pipeline status to `inference/control/status` topic
4. **Connection management**: Maintains persistent connection to MQTT broker with automatic reconnection

### Callback Interface

The Control Plane exposes callback properties that the controller assigns [adeline/app/controller.py290-301](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L290-L301):

|Callback Property|Triggered By|Controller Handler|
|---|---|---|
|`on_stop`|`{"command": "stop"}`|`_handle_stop()`|
|`on_pause`|`{"command": "pause"}`|`_handle_pause()`|
|`on_resume`|`{"command": "resume"}`|`_handle_resume()`|
|`on_metrics`|`{"command": "metrics"}`|`_handle_metrics()`|
|`on_toggle_crop`|`{"command": "toggle_crop"}`|`_handle_toggle_crop()`|
|`on_stabilization_stats`|`{"command": "stabilization_stats"}`|`_handle_stabilization_stats()`|

### QoS 1 Rationale

Control commands use QoS 1 (at-least-once delivery) to ensure critical operations like `stop` or `pause` are not lost due to network issues. This is essential for operational safety.

Sources: [adeline/CLAUDE.md27-31](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/CLAUDE.md#L27-L31) [adeline/app/controller.py278-301](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L278-L301)

## MQTTDataPlane

The `MQTTDataPlane` component [adeline/data/plane.py](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/data/plane.py) handles high-throughput result publishing with QoS 0 for performance.

### Responsibilities

1. **Detection publishing**: Publishes inference results to `inference/data/detections` topic
2. **Metrics publishing**: Publishes watchdog metrics to `inference/data/metrics` topic
3. **Watchdog integration**: Accepts `BasePipelineWatchDog` instance to access metrics [adeline/app/controller.py89](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L89-L89)
4. **Performance optimization**: Uses QoS 0 to prioritize throughput over guaranteed delivery

### Key Methods

|Method|Purpose|
|---|---|
|`connect()`|Establishes MQTT broker connection|
|`disconnect()`|Closes MQTT connection|
|`set_watchdog()`|Registers watchdog instance for metric access|
|`publish_detection()`|Publishes inference result as JSON|
|`publish_metrics()`|Publishes current watchdog metrics|
|`get_stats()`|Returns publication statistics (total published, errors)|

### Integration with Sinks

The Data Plane is wrapped by `create_mqtt_sink()` [adeline/data/sinks.py](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/data/sinks.py) which returns a callable suitable for the `multi_sink` pattern [adeline/app/controller.py92](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L92-L92)

Sources: [adeline/CLAUDE.md33-37](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/CLAUDE.md#L33-L37) [adeline/app/controller.py74-92](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L74-L92)

## InferencePipeline

The `InferencePipeline` class from the inference library is the core ML execution engine. It is initialized via two different patterns depending on configuration:

### Standard Pipeline Initialization

When ROI mode is `'none'`, the pipeline uses the default initialization pattern [adeline/app/controller.py268-276](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L268-L276):

```
InferencePipeline.init(
    max_fps=self.config.MAX_FPS,
    model_id=self.config.MODEL_ID,
    video_reference=self.config.RTSP_URL,
    on_prediction=on_prediction,
    api_key=self.config.API_KEY,
    watchdog=self.watchdog,
    status_update_handlers=[self._status_update_handler],
)
```

### Custom Logic Pipeline Initialization

When ROI mode is `'adaptive'` or `'fixed'`, the pipeline uses custom processing logic [adeline/app/controller.py238-245](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L238-L245):

```
InferencePipeline.init_with_custom_logic(
    video_reference=self.config.RTSP_URL,
    on_video_frame=self.inference_handler,  # Custom wrapper
    on_prediction=on_prediction,
    max_fps=self.config.MAX_FPS,
    watchdog=self.watchdog,
    status_update_handlers=[self._status_update_handler],
)
```

The `on_video_frame` parameter receives either `AdaptiveInferenceHandler` or `FixedROIInferenceHandler` [adeline/app/controller.py192-209](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L192-L209) which perform ROI cropping before inference.

### Pipeline Lifecycle

- `start()`: Begins automatic frame capture and inference [adeline/app/controller.py311](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L311-L311)
- `pause_stream()`: Temporarily halts processing without disconnecting [adeline/app/controller.py348](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L348-L348)
- `resume_stream()`: Resumes after pause [adeline/app/controller.py360](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L360-L360)
- `terminate()`: Stops pipeline and releases resources [adeline/app/controller.py333](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L333-L333)
- `join()`: Waits for pipeline threads to complete [adeline/app/controller.py503](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L503-L503)

Sources: [adeline/app/controller.py131-276](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L131-L276) [adeline/CLAUDE.md56-73](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/CLAUDE.md#L56-L73)

## ROI Strategy Components

The ROI (Region of Interest) system uses a factory pattern to create appropriate strategy implementations based on configuration.

### Factory Function

`validate_and_create_roi_strategy()` [adeline/inference/roi/__init__.py](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/inference/roi/__init__.py) creates the appropriate ROI state object:

- **Mode: `'adaptive'`** → Returns `AdaptiveROIState` [adeline/app/controller.py165-168](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L165-L168)
- **Mode: `'fixed'`** → Returns `FixedROIState` [adeline/app/controller.py165-168](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L165-L168)
- **Mode: `'none'`** → No ROI strategy created, uses standard pipeline

### ROI State Objects

ROI state objects maintain cropping regions and are updated by the `roi_update_sink` [adeline/app/controller.py218-219](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L218-L219):

|State Class|Behavior|
|---|---|
|`AdaptiveROIState`|Dynamically adjusts crop based on previous detections with temporal smoothing|
|`FixedROIState`|Uses static coordinates defined in configuration|

### Inference Handlers

When ROI strategies are active, inference handlers wrap the model execution:

|Handler Class|File Location|Purpose|
|---|---|---|
|`AdaptiveInferenceHandler`|[adeline/inference/roi/handler.py](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/inference/roi/handler.py)|Crops frame to adaptive ROI before inference|
|`FixedROIInferenceHandler`|[adeline/inference/roi/handler.py](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/inference/roi/handler.py)|Crops frame to fixed ROI before inference|

The handlers expose an `enabled` property that can be toggled at runtime via the `toggle_crop` command [adeline/app/controller.py389-397](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L389-L397)

### Runtime Control

The `_handle_toggle_crop()` method [adeline/app/controller.py375-397](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L375-L397) allows enabling/disabling adaptive ROI at runtime by modifying `inference_handler.enabled`. This only works when `ROI_MODE == 'adaptive'`.

Sources: [adeline/CLAUDE.md58-62](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/CLAUDE.md#L58-L62) [adeline/app/controller.py131-219](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L131-L219)

## Stabilization Strategy Components

Detection stabilization reduces flickering by requiring detections to persist across multiple frames before being confirmed.

### Factory Function

`create_stabilization_strategy()` [adeline/inference/stabilization/core.py](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/inference/stabilization/core.py) creates stabilizers based on configuration:

- **Mode: `'temporal'`** → Returns `TemporalHysteresisStabilizer` [adeline/app/controller.py114](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L114-L114)
- **Mode: `'none'`** → No stabilization, raw detections pass through [adeline/app/controller.py123-125](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L123-L125)

### Stabilization Sink Wrapper

When stabilization is enabled, `create_stabilization_sink()` [adeline/inference/stabilization/core.py](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/inference/stabilization/core.py) wraps the downstream MQTT sink [adeline/app/controller.py117-120](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L117-L120):

```
mqtt_sink = create_stabilization_sink(
    stabilizer=self.stabilizer,
    downstream_sink=mqtt_sink,
)
```

This creates a middleware layer that filters detections before they reach MQTT publishing.

### Temporal Hysteresis Strategy

The `TemporalHysteresisStabilizer` implements two-threshold detection confirmation:

- **High threshold** (e.g., 0.5): Confidence required for initial detection appearance
- **Low threshold** (e.g., 0.3): Lower confidence allowed for persistent detections
- **Minimum frames**: Number of consecutive frames required before confirmation
- **Max gap**: Maximum frame gap before track is considered lost

### Runtime Statistics

The `_handle_stabilization_stats()` method [adeline/app/controller.py399-434](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L399-L434) retrieves and logs statistics from the stabilizer:

- Total detected objects
- Total confirmed objects
- Total ignored objects (below threshold)
- Total removed objects (lost tracking)
- Active tracks
- Confirmation ratio
- Breakdown by object class

Sources: [adeline/CLAUDE.md64-69](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/CLAUDE.md#L64-L69) [adeline/app/controller.py94-125](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L94-L125)

## BasePipelineWatchdog

The `BasePipelineWatchDog` from the inference library monitors pipeline performance in real-time. It tracks:

- **FPS**: Frames processed per second
- **Latency**: Time between frame capture and inference completion
- **Status updates**: Pipeline health indicators

The watchdog is created by the controller [adeline/app/controller.py64](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L64-L64) and:

1. Passed to the `InferencePipeline` during initialization for internal monitoring
2. Connected to the `MQTTDataPlane` for metric publishing [adeline/app/controller.py89](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L89-L89)

When the `metrics` command is received, `data_plane.publish_metrics()` [adeline/app/controller.py371](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L371-L371) reads current values from the watchdog and publishes them via MQTT.

Sources: [adeline/app/controller.py64-89](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L64-L89) [adeline/CLAUDE.md36](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/CLAUDE.md#L36-L36)

## Component Interaction Sequence

This sequence diagram shows the complete interaction flow during setup, normal operation, and command handling. It maps each interaction to the actual method calls in the codebase.

Sources: [adeline/app/controller.py68-527](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L68-L527)

## Multi-Sink Pattern Integration

The controller uses the `multi_sink` function from the inference library to compose multiple output destinations. This pattern allows inference results to fan out to multiple sinks simultaneously without modifying the pipeline.

### Sink Composition

When ROI mode is `'adaptive'`, the controller creates three sinks [adeline/app/controller.py214-234](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L214-L234):

```
sinks_list = [mqtt_sink]

# Add ROI update sink for adaptive mode
roi_sink = partial(roi_update_sink, roi_state=self.roi_state)
sinks_list.append(roi_sink)

# Add visualization sink if enabled
if self.config.ENABLE_VISUALIZATION:
    viz_sink = create_visualization_sink(
        roi_state=self.roi_state,
        inference_handler=self.inference_handler,
        display_stats=self.config.DISPLAY_STATISTICS,
        window_name=window_name,
    )
    sinks_list.append(viz_sink)

on_prediction = partial(multi_sink, sinks=sinks_list)
```

### Sink Execution Order

When `on_prediction` is called with inference results, `multi_sink` executes each sink in sequence:

1. **Stabilization sink** (if enabled): Filters detections [adeline/app/controller.py117-120](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L117-L120)
2. **MQTT sink**: Publishes to Data Plane [adeline/data/sinks.py](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/data/sinks.py)
3. **ROI update sink**: Updates adaptive ROI state [adeline/inference/roi/handler.py](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/inference/roi/handler.py)
4. **Visualization sink**: Renders to OpenCV window [adeline/visualization/sinks.py](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/visualization/sinks.py)

This composition is fixed at initialization time based on configuration, but individual sinks can be enabled/disabled at runtime (e.g., `toggle_crop` for ROI).

Sources: [adeline/CLAUDE.md113-121](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/CLAUDE.md#L113-L121) [adeline/app/controller.py214-264](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L214-L264)

## Configuration Dependencies

All components depend on `PipelineConfig` [adeline/config.py](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py) for initialization parameters. The controller passes config values to each component during setup:

|Component|Configuration Used|
|---|---|
|`MQTTControlPlane`|`MQTT_BROKER`, `MQTT_PORT`, `CONTROL_COMMAND_TOPIC`, `CONTROL_STATUS_TOPIC`, `MQTT_USERNAME`, `MQTT_PASSWORD`|
|`MQTTDataPlane`|`MQTT_BROKER`, `MQTT_PORT`, `DATA_TOPIC`, `METRICS_TOPIC`, `MQTT_USERNAME`, `MQTT_PASSWORD`, `DATA_QOS`|
|`InferencePipeline`|`RTSP_URL`, `MAX_FPS`, `MODEL_ID`, `API_KEY`, `MODEL_CONFIDENCE`, `MODEL_IOU_THRESHOLD`|
|ROI Strategy|`ROI_MODE`, `CROP_MARGIN`, `CROP_SMOOTHING`, `FIXED_X_MIN`, `FIXED_Y_MIN`, `FIXED_X_MAX`, `FIXED_Y_MAX`|
|Stabilization|`STABILIZATION_MODE`, `STABILIZATION_MIN_FRAMES`, `STABILIZATION_MAX_GAP`, `STABILIZATION_APPEAR_CONF`, `STABILIZATION_PERSIST_CONF`|

The critical `disable_models_from_config()` function [adeline/config.py](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py) must be called before any component instantiation to prevent unwanted model loading. This is enforced at the module level in [adeline/app/controller.py25-28](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L25-L28)

Sources: [adeline/config.py](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py) [adeline/app/controller.py59-319](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py#L59-L319) [adeline/CLAUDE.md44-54](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/CLAUDE.md#L44-L54)