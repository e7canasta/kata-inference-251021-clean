# Initialization Sequence

Relevant source files

- [adeline/CLAUDE.md](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/CLAUDE.md)
- [adeline/__main__.py](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/__main__.py)
- [adeline/config.py](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py)

## Purpose and Scope

This document describes the critical initialization sequence of the Adeline inference system, explaining the precise order in which components must be initialized and why this order matters. The initialization sequence is designed to prevent model loading warnings, ensure proper configuration propagation, and establish reliable communication channels before the pipeline begins processing video frames.

For information about the configuration system itself, see [Configuration Reference](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/6-configuration-reference). For details on individual components being initialized, see [Component Overview](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/3.3-component-overview). For operational guidance on starting the system, see [Running Your First Pipeline](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/2.3-running-your-first-pipeline).

## Overview

The Adeline system follows a strict six-phase initialization sequence:

1. **Pre-Import Phase**: Model disabling via environment variables
2. **Configuration Loading Phase**: Reading `.env` and `config.yaml` files
3. **Safe Import Phase**: Importing inference modules after model disabling
4. **Component Initialization Phase**: Creating controller, planes, and pipeline
5. **Infrastructure Connection Phase**: Establishing MQTT connections
6. **Pipeline Startup Phase**: Beginning video processing

The most critical aspect of this sequence is that `disable_models_from_config()` **must execute before importing the `inference` module**. Violating this order results in attempting to load heavy ML models that are not needed, causing `ModelDependencyMissing` warnings and potential memory issues.

Sources: [adeline/CLAUDE.md44-51](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/CLAUDE.md#L44-L51) [adeline/config.py20-51](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L20-L51)

## Complete Initialization Flow

**Initialization Sequence Flow**: This diagram shows the complete initialization sequence from entry point to operational system. The critical ordering ensures model disabling happens before imports, configuration is available to all components, and infrastructure connections are established before processing begins.

Sources: [adeline/__main__.py1-8](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/__main__.py#L1-L8) [adeline/config.py1-206](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L1-L206) [adeline/CLAUDE.md44-51](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/CLAUDE.md#L44-L51)

## Phase 1: Pre-Import Model Disabling

### The Critical `disable_models_from_config()` Call

The `disable_models_from_config()` function is the **most critical** initialization step. It must execute before any imports of the `inference` module occur.

|Aspect|Details|
|---|---|
|**Function Location**|[adeline/config.py22-51](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L22-L51)|
|**Execution Timing**|Before `from inference import InferencePipeline`|
|**Purpose**|Prevent loading of heavy ML models not used by pipeline|
|**Mechanism**|Sets environment variables `{MODEL_NAME}_ENABLED=False`|
|**Default Models Disabled**|PALIGEMMA, FLORENCE2, QWEN_2_5, CORE_MODEL_SAM, CORE_MODEL_SAM2, CORE_MODEL_CLIP, CORE_MODEL_GAZE, SMOLVLM2, DEPTH_ESTIMATION, MOONDREAM2, CORE_MODEL_TROCR, CORE_MODEL_GROUNDINGDINO, CORE_MODEL_YOLO_WORLD, CORE_MODEL_PE|

### Why This Order Matters

**Model Disabling Order**: This diagram illustrates why `disable_models_from_config()` must execute before importing the inference module. The correct order prevents model loading attempts, while the incorrect order results in warnings and wasted resources.

### Implementation Details

The function reads the `models_disabled.disabled` array from `config.yaml` and sets corresponding environment variables:

```
# config.yaml example
models_disabled:
  disabled:
    - PALIGEMMA
    - FLORENCE2
    - QWEN_2_5
```

For each model name in the list, the function executes:

```
os.environ[f"{model}_ENABLED"] = "False"
```

If `config.yaml` does not exist, the function applies a conservative default list that disables all heavy models [adeline/config.py32-41](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L32-L41)

Sources: [adeline/config.py22-51](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L22-L51) [adeline/CLAUDE.md106-110](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/CLAUDE.md#L106-L110)

## Phase 2: Configuration Loading

### PipelineConfig Initialization

After model disabling, the `PipelineConfig` class loads all system configuration from two sources:

**Configuration Loading Flow**: The PipelineConfig class aggregates settings from environment variables (sensitive credentials) and YAML file (application settings) into a single configuration object used throughout initialization.

### Configuration Hierarchy

|Source|Content Type|Examples|
|---|---|---|
|`.env` file|Sensitive credentials|`ROBOFLOW_API_KEY`, `MQTT_USERNAME`, `MQTT_PASSWORD`|
|`config.yaml`|Application settings|Pipeline, models, MQTT, ROI, stabilization, logging|
|Defaults|Fallback values|Used when config.yaml missing (with warnings)|

The configuration loading process [adeline/config.py56-206](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L56-L206):

1. Check if `config.yaml` exists, raise `FileNotFoundError` if missing
2. Load YAML content with `yaml.safe_load()`
3. Extract pipeline settings: `rtsp_url`, `model_id`, `max_fps`, etc.
4. Extract model settings: `use_local`, `local_path`, `imgsz`, `confidence`
5. Load `ROBOFLOW_API_KEY` from environment (required if not using local models)
6. Extract MQTT broker settings: `host`, `port`, `topics`, `qos`
7. Load `MQTT_USERNAME` and `MQTT_PASSWORD` from environment (if available)
8. Extract ROI strategy settings with backward compatibility for legacy `adaptive_crop.enabled`
9. Extract detection stabilization settings: `mode`, `temporal`, `hysteresis`
10. Extract logging configuration: `level`, `format`, `paho_level`

Sources: [adeline/config.py56-206](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L56-L206) [adeline/CLAUDE.md45-54](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/CLAUDE.md#L45-L54)

## Phase 3: Safe Import Phase

After configuration loading and model disabling, imports of the `inference` module are now safe:

```
from inference import InferencePipeline
from inference.core.interfaces.stream.sinks import multi_sink
```

At this point, the `inference` module checks environment variables like `PALIGEMMA_ENABLED` and skips loading disabled models, preventing warnings and reducing memory usage.

Sources: [adeline/CLAUDE.md106-110](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/CLAUDE.md#L106-L110)

## Phase 4: Component Initialization

### Controller Creation

**Component Initialization**: The InferencePipelineController orchestrates creation of all system components, using the configuration object to parameterize each component's behavior.

### Component Creation Order

The controller initializes components in this specific order [adeline/app/controller.py](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/app/controller.py):

1. **MQTTControlPlane**: Created with `client_id="inference_control"`, QoS 1 for reliable command delivery
2. **MQTTDataPlane**: Created with `client_id="inference_data"`, QoS 0 for high-throughput data publishing
3. **ROI Strategy**: Factory creates strategy based on `config.ROI_MODE` (none/adaptive/fixed)
4. **Stabilization Strategy**: Factory creates strategy based on `config.STABILIZATION_MODE` (none/temporal)
5. **InferencePipeline**: Initialized with model settings, ROI strategy, and stabilization strategy
6. **Signal Handlers**: Registered for SIGINT and SIGTERM to enable graceful shutdown

### Factory Pattern Initialization

Both ROI and stabilization strategies use factory functions that read configuration and instantiate the appropriate strategy class:

|Factory Function|Configuration Key|Possible Strategies|
|---|---|---|
|`create_roi_strategy()`|`config.ROI_MODE`|`none`, `adaptive`, `fixed`|
|`create_stabilization_strategy()`|`config.STABILIZATION_MODE`|`none`, `temporal`|

The factories enable strategy selection without code modification, implementing the configuration-driven architecture principle. See [Factory Pattern for Strategies](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/4.2-factory-pattern-for-strategies) for detailed information on factory implementations.

Sources: [adeline/CLAUDE.md56-73](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/CLAUDE.md#L56-L73) [adeline/config.py150-200](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L150-L200)

## Phase 5: Infrastructure Connection

### MQTT Connection Sequence

After component initialization, MQTT connections are established before starting the pipeline:

**MQTT Connection Sequence**: Control plane connects with QoS 1 and subscribes to commands, while data plane connects with QoS 0 for publishing only. Both use unique client IDs to prevent conflicts.

### Connection Parameters

|Plane|Client ID|QoS|Subscriptions|Publications|
|---|---|---|---|---|
|Control Plane|`inference_control`|1 (reliable)|`inference/control/commands`|`inference/control/status`|
|Data Plane|`inference_data`|0 (performance)|None|`inference/data/detections`, `inference/data/metrics`|

The control plane uses QoS 1 to ensure commands like `stop` or `pause` are reliably delivered, while the data plane uses QoS 0 to prioritize throughput for high-volume detection data. This QoS strategy is fundamental to the control/data plane separation architecture. See [MQTT Communication Architecture](https://deepwiki.com/care-foundation/kata-inference-251021-clean2/3.2-mqtt-communication-architecture) for detailed explanation of QoS choices.

Sources: [adeline/CLAUDE.md102-105](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/CLAUDE.md#L102-L105) [adeline/CLAUDE.md27-37](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/CLAUDE.md#L27-L37)

## Phase 6: Pipeline Startup

### Final Initialization Steps

After all infrastructure connections are established, the pipeline begins processing:

**Pipeline Startup**: The final initialization phase begins video processing, connects to RTSP stream, and starts publishing detections and metrics.

### Operational State Verification

Once the pipeline starts successfully:

1. RTSP connection established to video source
2. First frame received and processed
3. Model inference executed on first frame
4. ROI strategy applied (if configured)
5. Stabilization strategy applied (if configured)
6. First detections published to MQTT data plane
7. Status update published to MQTT control plane: `{"state": "RUNNING"}`
8. Metrics watchdog begins periodic publication
9. System enters steady-state operation

At this point, the system is fully operational and ready to receive control commands via MQTT.

Sources: [adeline/CLAUDE.md1-143](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/CLAUDE.md#L1-L143)

## Initialization Checklist

The following checklist summarizes the critical initialization requirements:

|Phase|Requirement|Consequence if Violated|
|---|---|---|
|Pre-Import|`disable_models_from_config()` called before `from inference import`|`ModelDependencyMissing` warnings, slow startup, excessive memory usage|
|Configuration|`.env` file exists with required keys|`ValueError` for missing `ROBOFLOW_API_KEY` (if not using local models)|
|Configuration|`config.yaml` exists|`FileNotFoundError` with instructions to create from example|
|Component Init|Unique MQTT client IDs used|Client ID conflicts, connection failures|
|Infrastructure|MQTT broker running and accessible|Connection failures, no control or data communication|
|Startup|RTSP stream accessible|Pipeline fails to start, no video processing|

## Error Handling During Initialization

Each initialization phase includes validation that can halt startup if critical requirements are not met:

|Phase|Validation|Error Raised|
|---|---|---|
|Pre-Import|Config file readable (optional)|None (uses defaults)|
|Configuration Loading|`config.yaml` exists|`FileNotFoundError`|
|Configuration Loading|`ROBOFLOW_API_KEY` present (if needed)|`ValueError`|
|Component Init|MQTT broker accessible|`ConnectionError` (paho-mqtt)|
|Pipeline Start|RTSP stream accessible|Runtime exception from inference library|

The system intentionally fails fast during initialization rather than running in a degraded state, following the principle of explicit failure over implicit misconfiguration.

Sources: [adeline/config.py67-100](https://github.com/care-foundation/kata-inference-251021-clean2/blob/9a713ffb/adeline/config.py#L67-L100)